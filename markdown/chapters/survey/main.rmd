

\chapter{Data collection}

\section{Background on survey design}

As explained by @wiley2004, a survey is a means of obtaining quantitative information regarding opinions and experiences of the respondents in order to explore the views of the target population as a whole. In this book, a survey is noted as a "systematic" method of collecting data, where the author states that the word "systematic" is deliberately used 
in order to separate surveys from other methods of information collection. "systematic" is defined by the Collins English Dictionary as something that \textit{"is done according to a fixed plan, in a thorough and efficient way"} [@collins-systematic], and this reflects the manner in which surveys are created in accordance with a given system, where methods for distribution, implementation and analysis are defined under a pre-determined structure. The survey will be delivered to potential respondents in the target population, who will then be asked to complete a series of standardised questions, or questions for which the question ordering and wording is identical for every respondent, unless different formats are to be used to research purposes. It is once again discussed by @wiley2004 that standardised questioning was not always the norm; most interviewers would more likely have a list of objectives, and each interviewer would formulate and word questions based around these. It was discovered that question wording can have a drastic effect on respondents' answers.

Whether or not the survey is 'thorough' and 'efficient' depends heavily on the survey structure and design. Designing an effective, systematic survey involves balancing efficiency with completeness, creating a survey that can obtain as much information as possible whilst not boring or fatiguing participants, which can lead to non-response and measurement errors due to participants skipping questions or selecting answers at random. A well-designed systematic survey has the capacity to yield large amounts of both qualitative and quantitative information regarding the research topic while minimising these errors.

There exist a variety of methods for delivering a survey, such as self-completed questionnaires and interviewer-administered interviews. Depending on the aims of the study, there will be advantages and disadvantages to each method. There may also be times when a combined approach is helpful in gathering the necessary information. 
The first method of surveying, a questionnaire, may consist of either physical paper forms that are mailed or handed out to people within the target population, or in an online format. As discussed by @brace2004, this form of surveying constitutes a method of indirect communication between the respondent and researcher, in effect a non-verbal conversation in which the respondent is replying to the researcher's questions. The non-face-to-face aspect of this method can be beneficial in terms of anonymity; an anonymous respondent is more likely to be honest in their answers than a respondent for whom the identity is known. As a result, an anonymous questionnaire can mitigate errors that may be caused by respondents fearing judgment of their answers. It is also possible to administer a large number of these questionnaires in a short period of time since they are self-administered, and thus
constraints such as the number of interviewers or time taken to administer the survey has less effect on the amount of information obtained.  

There are, however negatives to this questionnaire method. In his book, Brace discusses the way in which question wording must be very carefully thought about when using this method of indirect conversation, for reasons such as there being no way to correct participant misunderstanding of questions. Additionally, the fact that the researcher and participant never come into contact may allow the researcher to write questions without considering the human nature of the participants; it is easy to become absorbed in attempting to gather information and fall into forgetting that long-winded or complicated questions may bore or confuse respondents, leading to poorer quality responses. Similarly including too many questions in the questionnaire may lead to response errors for the same reasons. It is then crucial to be as clear and concise as possible in question wording, leaving little room for interpretation. This type of survey is also a very static medium; it does not allow for much expansion on participants' answers, with reasoning behind answers unknown unless specifically requested, which again could add to respondent fatigue and affect quality of response.

We can attempt to implement some dynamic discussion into a questionnaire in the form of 'open-ended questions', mentioned above as specifically requesting reasoning behind answers. A questionnaire is composed of two types of questions; closed-ended questions, for which the respondent selects their answer from a given set of potential responses, and open-ended questions, in which the participants are able to write their answers in a free-form format. 
Closed-ended questions are very good for obtaining quantitative data that may be easily categorised and counted, which is useful for gathering empirical evidence in order to form objective conclusions regarding the sample population. 

Open-ended questions are generally used where more expansion may be required in addition to the closed-form answer, or if using a closed-form question would limit the answer range. The Leibniz Institute for the Social Sciences [@leibniz] provides guidance on open-ended questions, in which the occasions for using open-ended questions are outlined as:

\begin{itemize}
   \item "knowledge measurement"; with with multiple choice, respondents would have a chance of guessing the correct answer, and thus this would be a sub-optimal way to measure raw knowledge
   \item "Unknown range of possible answers"; multiple choice may be limiting for certain questions, and may cause the researcher to miss important information 
   \item "Avoidance of excessively long lists of response options"; if there is a known range of answers, but this range is very large, it may overwhelm respondents to see all of these as options
   \item "Avoidance of directive questions"; certain questions may have options based on the researcher's own opinions, and thus have the potential to direct the participant in a certain direction, and may not reflect the participants' true views. This links to "unknown range of answers" in that the researcher may incorrectly assume the potential range of answers and thus the given options may not cover the respondents' true opinions.
   \item "Cognitive pretesting", which covers instances such as ensuring the question was understood correctly.
\end{itemize}

To summarise, open-ended questions are useful when either there is not enough information to set a standardised range of potential responses or if more information is needed after a closed-ended response.

A method of surveying that is, by design, more dynamic is an interview. An interview may be structured, semi-structured or structured and each of these have a different set of features that distinguish them from one another. Structured interviews, as by the name, are rigid in nature and comprise of a vocal conversation in which the interviewer has a specific set of questions from which the discussion does not deviate. The slightly less rigid semi-structured interview is similar, but slight deviation from the plan is allowed in order to explore new avenues and ideas that might not be found with a structured interview, but the interviewer will still have a set of specific questions for which to obtain responses. For the most flexible of the three, the unstructured interview, the interviewer will tend to follow a loose plan of what they wish to explore rather than a strict question schedule, with the discussion led by the respondent's answers.

Phone calls and other forms of interview-based survey allow the interviewer to form a personal connection with the survey participant, which can be especially helpful for a company's image if the interviewer is particularly professional or charismatic. Additionally, while the interviewer will still be limited to asking the pre-set questions, the format of such a survey can 
be considered semi-structured and with much more room for interpretation. This can lend itself to gaining additional insights that may not have otherwise been gathered from a more closed-form paper or online survey. Additionally, the more open format can negate any error as a result of participants misinterpreting questions due to the interviewer's ability to immediately 
clarify on any misunderstandings. This type of survey also provides an instant response, which is beneficial if there is only a short time frame available in which to gather information. 

However, there are also shortfalls to an interview-based survey method. For instance, although a charismatic interviewer can positively impact the image of whoever is conducting the survey, this could also lead to biases, such as the respondent answering in a way they feel will please the interviewer. Additionally, the image of the organisation could potentially be tainted if the interviewer appears rude or unprofessional, alongside potentially providing bias in the opposite direction. As well as this, telephone surveys are likely to be interpreted as a telemarketing scheme, and thus potentially have a negative impact on the number of willing respondents. The reduced anonymity of this type of survey may also create bias in the way of participants avoiding making statements that could be deemed socially unacceptable, or that they feel they may be judged for, and therefore may not provide answers accurate to their true line of thought.

The UK Household Longitudinal Study [@longitudinal] is an ongoing study and an example of implementation of a combined use of the above mentioned surveying methods. Initially, in 'wave 1' of the study, a sample of 40,000 households in the UK were selected to be surveyed on a yearly basis. The survey involves all members of each selected household, overall comprising of around 100,000 individuals, and asks them a range of questions regarding areas such as family life, income, employment and health. The study consists of a self-administered youth paper questionnaire given to respondents ages 10-15, and an interview for those aged 16 and up. This split in age demographic allows some questions to be omitted from the youth survey, such as those about income and employment, and some to be added such as about pocket money habits and 'future intentions', as the website states. Giving the youth respondents a paper questionnaire may help obtain more useful or relevant answers, as the respondent may be more comfortable with this than being interviewed by an adult. The youth questionnaire is also shorter, which could perhaps just be a result of many questions not being relevant to this demographic, or it could be a conscious decision, but either way this with help to ensure the young respondent doesn't lose interest and potentially incur bias in their answers due to either rushing to finish the survey or not paying attention. The adult survey also includes a section specific to 16-21 year olds. The surveys contain a standardised set of core questions asked each year alongside a set asked every other year. The reasoning behind this is given to be that this study has a very large scope, asking about many aspects of each respondents' life, and so it becomes inefficient and counterproductive to include all questions every year since, as mentioned previously, the longer a survey is, the more likely a respondent is to get bored or mentally fatigued. The fact that the adult survey is administered in an interview also means that there may be limits on the amount of time the survey can take, as interviewers may have to get through a certain number of respondents in a day, additionally to the interviewer potentially also becoming fatigued. If the interviewer is fatigued, their tone and how they hold themselves may change, and potentially cause a subconscious bias in how the respondent answers the questions. 


\section{Specific goals of survey tool for this study}
While visualisations can be a very useful tool for understanding data, they also have the potential to be highly misleading. This section of the study will explore how modifying certain aesthetic features of visualisations can impact perception and interpretation of data, and how these modifications can be exploited in order to mislead the observer. Misleading visualisations may be created in an effort to deliberately influence the viewers' perceptions, or accidentally as a result of poor practice and knowledge surrounding data visualisation. In either case, visualisations have the ability to communicate different messages and stories depending on how they present the data to the observer. 

The specific aim of the survey is to test whether altering y-axis scaling, bar width, bar grouping method and colouring will have an impact on single data value interpretation and subjective interpretation of differences in data values. 

\section{Survey Design}
The survey design will be inspired by the papers discussed in the previous literature review, all of which investigate how different aesthetic and design choices have the potential to mislead the observer or alter perception.

Following this, questions included in part 1 the survey will focus on gauging whether altering the y-scale to be truncated or logarithmic has an effect on user perception of difference in data point values, for both bar and line plots. The respondents will be asked to gauge both individual values and differences in values, with the former providing an open answer box in which the may type their answer to allow for maximum freedom and obtain their true observation, unimpeded by the bias of having a specific set of numbers to pick from when their true observation may lie outside this range. The question for gauging difference perception follows @claire-obrian and @YANG2021 in using a numbered scale with numbers representing a range from not much difference up to a large difference. The @YANG2021 method of a 7-point scale was employed here. From these papers, it is hypothesised that the truncated scale will cause respondents to overestimate differences between data values, and the logarithmic scale will be hypothesised to result in underestimation.

Additionally, stacked bar charts will be investigated, showing a comparison between using the stacking method as opposed to a grouped bar plot. Based on reviewing the literature, part 3 of the survey will include questions with the objective of testing standard stacked against grouped bar charts, alongside questions relating to the colour palettes used in depicting the different groups. We aim to test which colour palette is preferred in terms of aesthetics as well as ease of interpretation and reading.

The last two parts of the survey, noted henceforth as 'Sales - part 1' and 'Sales - part 2', explore the different y-axis scalings with respect to line plots, but for these, as opposed to the bar plots, the default was a truncated axis. The three plots investigated will consist of line plots relating to time series data for two fictitious companies. One will display each of the two lines on separate plots with the default axis, one will show both on the same plot with the default axis, and finally one with both on the same plot but with a zeroed axis. It is hypothesised that a difference in value for two time points will be perceived as smaller fopr the zeroed axis, and larger for the separated plots.

As discussed in @Peytchev_Peytcheva_2017, too long a survey can result in higher measurement error due to factors such as waning interest or mental fatigue of respondents, resulting in careless responding and non-response. This is also further explored in @brower, whereby a study is carried out to determine causes of careless responding, and specifically looks at questionnaire length and participant disinterest. The study performed in this work provides evidence that longer survey length can have a detrimental affect on careless responding; a long survey may make participants more likely to respond carelessly, and this must be considered when designing an effective and efficient survey. An additional conclusion states that participant interest in the survey content could have an effect, but also that evidence is less supported for this claim. There is significant enough evidence, however, to say that this should also be considered when designing the survey. 

The @Peytchev_Peytcheva_2017 paper explains that a 'split survey' design, where each respondent is only asked to answer a selection of questions from the whole set, is effective in reducing error while gathering large amount of information, however this will not be employed here. The reasoning for this is that there will already be a set of 12 different surveys being sent, and creating further splits could potentially lead to much too small sample sizes and thus inconclusive results. Additionally to this, the paper investigates how placement of questions in the survey can affect responses, concluding that questions asked later in the survey are more susceptible to bias, which tracks with the conclusion of survey length being a cause of careless responding; the longer a participant is taking a survey for, the more likely they are to start being careless with responding. 

Due to this, the survey was designed to last in the range of approximately 15-20 minutes, as suggested in @length. One paper [@burdenpercep] explores the pecieved burden of a survey on the participant, and performs a study whereby respondents were assigned a questionnaire, but given one of two different time estimates, for which the true length of the survey lay between. It was found that more people started the survey with the lower estimated completion time, but more also dropped out. However, the time at which respondents dropped out did not significantly differ in the two groups. In order to obtain maximum response, it is wise to as accurately as possible disclose the true survey length, and even slightly over-estimate in the disclosure.  

With regard to the interest factor, the survey was designed with engaging respondents. The topic of the majority of the survey was chosen to be data relating to the television show \textit{American Ninja Warrior}, as this could be subjectively viewed as a 'more interesting' topic than seemingly meaningless numbers. The survey was administered to a test subject, who commented that they found this topic interesting, with the additional comment that perhaps some pictures of the Ninja Warrior obstacles would be nice, however was not employed. The survey also took this respondent about 20 minutes to complete. 

Although the content of the surveys for this study is not likely to be controversial or highly personal, anonymity is still important as the participants could otherwise potentially feel pressure to give a 'correct' answer, given the mathematical nature of the questions. As mentioned prior, anonymity here means that this pressure is potentially reduced and thus the relevant measurement bias may be mitigated. Additionally to the more technical visualisation questions, respondents were asked a series of demographic questions such as age, degree subject (if applicable),  and whether they are colourblind or have any disorders that my affect visual processing. Additionally, three Likert scaled questions relating to well they would rate their spatial, observational and numerical skills. The @YANG2021 paper, which explores the truncation effect of barplots, looks at graph literacy and its relation to perception, and hypothesises that those undertaking quantitative subjects at PhD level would be less impacted by the truncation effect as compared to humanities PhD students. It was found that the truncation effect did impact both groups, but those in quantitative fields had their perception marginally less affected. Thus the degree subject question was included to explore if this has an effect here. In relation to the visual processing and colorblindness questions, these are again included to test whether they have any significant impact on perception, as it may be important to consider these factors when creating visualisations to ensure they are accessible to all, and the study will examine the potential impact of such disorders.
 
The set will consist of two groups of surveys, which will be identical up to the visualisation package used. Particularly, one group will contain visualisations made with R's ggplot2, the next with matplotlib from Python. These surveys will be distributed to the general public by sharing links on social media platforms such as Facebook. The reasoning behind creating two separate surveys in different languages is to ascertain whether the language used influences the interpretation. Within the groups there are 6 surveys, with each altering the order of visualisations shown in part 1 to assess the perception of each plot type without reference or comparison to another, and the same with part 2. in Part 3, each of the 6 used one of 3 colour palettes as the main colour, and another as a comparitor to test which the preferred colour palette is and which respondents find easier to read and interpret. Note however that, while both languages were intended to be as close to default as possible, the ggplot visualisations were made such that the theme `theme_classic` was applied, as this is mirrors the Python format in terms of the absence of grid lines.

\section{Creating the Visualisations}

See appendix for the code and figures of the visualisations. The R visualisations were created using R version 4.0.2 [@R] using ggplot2 version 3.3.3 [@ggplot]. The Python visualisations were made using Python version 3.7.4 [@py] with pyplot from matplotlib version 3.3.3 [@matplot]. 

\subsection{The Data}
The visualisations for the survey were created with inspiration from the papers discussed above. The bar plots were created using a data set regarding the history of obstacles used over 10 seasons of \textit{'American Ninja Warrior'} [@ANW]. Each row of the data represents a single instance of an obstacle being used, and each instance has variables as specified in the below table.

\begin{center}
```{r}
vars <- c("season", "location", "round_stage", "obstacle_name", "obstacle_order")
expl <- c("Season in which instance occured", 
          "Location of use", 
          "Stage of competition in which instance occured", 
          "Name of the obstacle", 
          "Order in which the obstacle was placed in the course")
mat <- matrix(NA, 5, 2)
mat[,1] <- vars
mat[,2] <- expl

colnames(mat) <- c("Variable Name", "Explanation")

kable(mat)
```
\end{center}

This data was manipulated in R to produce a data frame containing the count of the number of times each obstacle was used over the course of the whole ten seasons. For the stacked and grouped bar plots, a data frame was produced, once again in R, containing columns 'obstacle' and 'stage', where 'obstacle' is a vector containing the name of each obstacle repeated the number of times it was used, and 'stage' similarly contains the names of all the stages of the competition, with each repeated the number of times it appeared. For example, Salmon Ladder was used 41 times, and thus is also repeated this many times, and there are 41 entries in the 'stage' vector corresponding to this. For the python version, the frequency tables were created manually.

The data for the time series plots was taken from the data set `BJsales` in the base R package `datasets` [@R]. This data consists of a single vector of values with 150 entries, where each entry corresponds to a measurement taken at some arbitrary time point. Four subsets were taken from this data such that a start index was selected, and then this entry and the 11 following consecutive entries were extracted. The vectors were put into a data frame with the time steps set as months, giving a year of sales data for four fictional companies. This again was used to manually create a data frame in Python. To select the starting index, several seeds were tested for random selection, and four seeds were selected that would create plots to best test the hypotheses.

\subsection{The Bar Plots}
As explained before, the bar plots for part 1 were made such that one uses the default axis scaling, one uses a truncated axis, and one uses a logarithmically-scaled axis. It is worth noting that in R attempting to truncate the bar plot itself does not work; the bar must start at the zero tick mark otherwise the bars do not show up. To get around this issue, the data itself was truncated before applying to a bar plot with the tick labels then altered to fit the truncation, using intervals of 10 as in the default plot. Python, on the other had, will perform the truncation without this issue and defaults to steps of 2.5, which could affect the reading of values. For the logarithmically scaled plots, R by default starts at 1 and uses a non-standard form notation with tick labels of 1, 3, 20, 30. Python does use standard form and has labels 0, $10^0$ and $10^1$, starting at zero. The Python scale starting at zero was before mentioned as potentially misrepresenting the data. The height gauging of the R plot could maybe be impacted by the scale starting at 1. The default for the Python control plot scaling was more granular than the R, with steps on 5 as opposed to 10. The control scales for both languages have a range [0, 40], and [20, 40] for the truncated plots. There were 4 bars corresponding to 4 of the most used obstacles, arranged in descending order.

The next part plays with the aspect ratio of the plots. In order to keep this accurate, the plots were saved within the code as opposed to saving from the viewing window. The default aspect ratio for the ggplot is 1/1 for height to width, and using pyplot.gca() and comparing to the default we see that the default for Python using this method is 0.1. For the 'wide' plot, the aspect ratios are halved to 0.5/1 and 0.05, respectively. For the narrow, the aspect ratios were doubled to 2/1 and 0.2. Note that the aspect ratios include the entire plotting area, including labels and titles. These plots contained 7 bars as opposed to the 4, but were still arranged in decending order.

The plots in the third part of the survey were the stacked and grouped plots. The three colour schemes were the package default, a greyscale, and the colurblind-friendly Viridis palette [@viridis]. The obstacles here were the same 4 as displayed in part 1, but with the added colours for the competition rounds. The default axis ratios here mean that the R plots appear taller in comparison to their width than the Python plots, due to the legends. 

\subsection{The Line Plots}
The plots for part 1 of this show the false sales data in the form of time series line plots, where the x-axis displays the months and y-axis shows number of sales. In the R version, the x-axis displays the 12 months in words, whereas the x-axis of Python version numbers the months and plots them in intervals of 2 months. This was an unintentional error on the part of the designer, however could be used to draw conclusions regarding how the two systems differ; monthly ticks in words or bi-monthly numbers. The plots in sales- part 2 were created very similarly, just with two different start indices.



\section{The Survey}
This section will discuss the specific survey questions and explain the differences in plot ordering and colour schemes between survey versions. Google forms was chosen as the medium for delivering the survey, as it is a free service and provides easy way to send out survey links and automatically compiles responses in a Google sheet along with time stamps, which can be exported to csv for analysis. To randomly assign each participant a survey, a javascript code was created to link to a landing page, which redirected the participant randomly to one of the 12 surveys. As time progressed it was possible to see how many respondents were taking each survey, and it was possibly to alter the Javascript accordingly to ensure each survey had an approximately even number of respondents. The survey was set such that each page contained a single question with a set of related sub-questions and only the plots relevant to these sub-questions, to prevent participants scrolling through the survey and seeing other figures which may alter their perception. This can also be used to analyse the effect of seeing other plots on perception of the plots following.


\subsection{Demographic Questions}

As discussed, the questions below are used to assess whether these factors have an impact on graph literacy and graph perception.

\begin{itemize}
    \item Please enter your age (Open)
    \item If you are a university student or past university graduate please specify your area of study. (Drop down box: Science, Technology, Engineering, Maths, Arts, Social Sciences, Humanities, Business, N/A, Other (please specify))
    \item How strongly do you agree with each of the following statements? (Linear scale with 1 - 5, 1=strongly disagree, 5=strongly agree)
    \item  - I have good spatial awareness skills 
    \item  - I have good observational skills 
    \item  - I have good numerical skills 
    \item Are you colourblind? (Checkbox: Yes, No, Prefer not to answer)
    \item Do you have any disorders that may affect visual processing? (this could be a general visual processing disorder 
    or dyslexia, dyscalculia, ADHD etc)
    ((Checkbox: Yes, No, Prefer not to answer))
\end{itemize}


\subsection{American Ninja Warrior - Part 1}
The questions regarding each of the three bar plots were as follows:

\begin{itemize}
  \item Approximately many times would you say the 'Salmon Ladder' was used? (Open)
  \item Approximately how much more than 'Log Grip' would you say 'Salmon Ladder' was used? (1-7 scale)
  \item Approximately how much more than 'Quintuple Steps' would you say 'Salmon Ladder' was used? (1-7 scale)
  \item In your opinion, approximately how many times would you say 'Log Grip' was used, as a percentage of the number of times 'Salmon Ladder' was used? (Open)
\end{itemize}

Here, the two questions with the difference rating scale are used to assess whether having the bars next to each other vs on opposite ends of the plot has an effect on the difference in rating when comparing the responses for each of the plots. 

The table below shows all the permutations of the three plot types, and which questionnaire version they appear in.

\begin{center}
```{r}
version <- c("V1", "V2", "V3", "V4", "V5", "V6")
question <- c("Q1", "Q2", "Q3")

order <- matrix(NA, 6, 3)

order[1,] <- c("Control", "Log", "Truncated")
order[2,] <- c("Control", "Truncated", "Log")
order[3,] <- c("Log", "Control", "Truncated")
order[4,] <- c("Log", "Truncated", "Control")
order[5,] <- c("Truncated", "Control", "Log")
order[6,] <- c("Truncated", "Log", "Control")

rownames(order) <- version

kable(order, col.names = question)
```
\end{center}

The table shows that, for example, in version 1, the control plot was shown in question 1, the log-scaled in question 2 and the truncated in question 3.

\subsection{American Ninja Warrior - Part 2}
The questions regarding each of the three bar plots were as follows:

\begin{itemize}
  \item How large would you say the difference between 'Jumping spider' and 'Salmon Ladder' is? (1-7 scale)
  \item How large would you say the difference between 'Log Grip' and 'Floating Steps' is? (1-7 scale)
  \item How many times would you say 'Floating Steps' were used? (Open)
\end{itemize}

Similar to part 1, the below table gives all permutations of the three plot types.
\begin{center}
```{r}
version <- c("V1", "V2", "V3", "V4", "V5", "V6")
question <- c("Q1", "Q2", "Q3")

order <- matrix(NA, 6, 3)

order[1,] <- c("Default", "Narrow", "Wide")
order[2,] <- c("Default", "Wide", "Narrow")
order[3,] <- c("Narrow", "Default", "Wide")
order[4,] <- c("Narrow", "Wide", "Default")
order[5,] <- c("Wide", "Default", "Narrow")
order[6,] <- c("Wide", "Narrow", "Default")

rownames(order) <- version

kable(order, col.names = question)
```
\end{center}
Questions regarding comparisons between the plots were then administered as follows, while showing respondents all of the three plots on a single page.

\begin{itemize}
  \item Which of the three bar charts do you find most aesthetically pleasing? (Multiple choice with options "A", "B" or "C")
  \item Which bar chart do you feel is easiest to read and interpret? (Multiple choice with options "A", "B" or "C")
  \item Which bar chart do you find hardest to read and interpret? (Multiple choice with options "A", "B" or "C")
\end{itemize}


\subsection{American Ninja Warrior - Part 3}
This part explored the differences in perception for stacked and grouped bar charts, alongside colour preferences. This part had 4 questions, with the first two asking about the stacked and grouped bar plots, with either the stacked first or grouped first. 

The first two sub-questions are given below.

\begin{itemize}
  \item How many times would you say 'Floating Steps' were used in the Finals (Regional/City) rounds? (Open)
  \item How many times would you say 'Log Grip' was used in the Finals (Regional/City) rounds? (Open)
\end{itemize}

The next question is "Please select the statement you feel applies to the bar chart above." and consists of a multiple choice answer with the following options: 
\begin{itemize}
  \item 'Log Grip' was used MORE in Finals (Regional/City) rounds than in Qualifying (Regional/City) rounds.
  \item 'Log Grip' was used Less in Finals (Regional/City) rounds than in Qualifying (Regional/City) rounds. 
  \item 'Log Grip' was used an EQUAL number of times in Finals (Regional/City) rounds and Qualifying (Regional/City) rounds.")
\end{itemize}

This is followed by another mulitple choice question, given as Which obstacle do you think was used MORE in Finals (Regional/City) rounds, 'Log Grip' or 'Floating Steps'?, with the following options:
\begin{itemize}  
  \item 'Log Grip'
  \item 'Floating Steps'
  \item They were used the same amount of times
\end{itemize}

After answering these questions for both plot types, the respondents were shown both on the same page and asked to select which of the two they found easier to read and interpret, and were then shown the stacked bar plot in two different colour palettes; the one used for the questions so far and a comparitor, with the questions below.

For the stacked vs grouped comparison:
\begin{itemize}
  \item Which bar chart do you feel is easiest to read and interpret? (Multiple choice with options "A", "B", "C")
\end{itemize}

For the colours comparison:
\begin{itemize}
  \item Which colour scheme do you find most aesthetically pleasing? (Multiple choice with options "A", "B", "C")
  \item Do you feel that one of the colour schemes makes it easier to read and interpret the data than the other? If so, please select which one. (Multiple choice with options "No", "Yes, A is easier", "Yes, B is easier")
\end{itemize}

For this part, survey versions 1, 2 and 4 showed the stacked bars first, followed by the grouped, and versions 3, 5 and 6 displayed the grouped first. It is shown in the below table which colour schemes were used in each survey.

\begin{center}
```{r}
prim <- c("Viridis", "Default", "Default", "Greyscale", "Viridis", "Greyscale")
comp <- c("Default", "Viridis", "Greyscale", "Default", "Greyscale", "Viridis")
vers <- c("V1", "V2", "V3", "V4", "V5", "V6")

mat <- matrix(NA, 6, 3)

mat[,1] <- vers
mat[,2] <- prim
mat[,3] <- comp

colnames(mat) <- c("Version", "Main colours", "Comparitor")

kable(mat)
```
\end{center}

\subsection{Sales - Part 1}
The respondents then moved onto part 1 of the sales section of the survey, in which they are asked to once again give subjective opinions regarding the y-axis scaling, but this time relating to time series line plots. 

Once again, the same set of questions is asked for each plot which consist of, firstly, a two-row multiple choice grid, with each row relating to one of the companies. Respondents were asked the question "How much would you say sales of each company increased between January and December?" and were to give a response on the 7-point scale. 

The ordering of the plots for each version number are given below.

\begin{center}
```{r}
version <- c("V1", "V2", "V3", "V4", "V5", "V6")
question <- c("Q1", "Q2", "Q3")

order <- matrix(NA, 6, 3)

order[1,] <- c("Separated", "Truncated", "Zeroed")
order[2,] <- c("Separated", "Zeroed", "Truncated")
order[3,] <- c("Truncated", "Separated", "Zeroed")
order[4,] <- c("Truncated", "Zeroed", "Separated")
order[5,] <- c("Zeroed", "Separated", "Truncated")
order[6,] <- c("Zeroed", "Truncated", "Separated")

rownames(order) <- version

kable(order, col.names = question)
```
\end{center}

The second question was "How large would you say the drop in sales between April and July of Company A  is?", which once again was rated based on the 7-point scale.


\subsection{Sales - Part 2}

The final part of the survey showed zeroed and truncated plots once again, for two different fictitious companies, this time with the intention of gaining an overall view. For each of the two, each respondent was asked a single 7-point scale rating question; "Based on the above graph, how large would you say the difference is between the number of sales Company C makes and the number of sales Company D makes?".

