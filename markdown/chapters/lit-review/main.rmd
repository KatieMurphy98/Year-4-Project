
\chapter{Literature Review}

\section{Introduction}
Data visualisation is a method of conveying data in an easily digestible manner through graphics. It is an important aspect of data presentation and allows key information to be quickly identified by the observer. Very many subject areas rely on such visualisations to relay messages that may get lost or have less impact when presented as written word or raw numbers. 

As stated by the creator of the R \textit{ggplot2} package, Harvey Wickham, \textit{"it is useful tothink about why we create visualizations: not to createpretty pictures, but to better understand our data."} [@ggplot2]. This summarises the key objective of visualisation; creating figures that display the data accurately in an aesthetic manner, giving non-misleading messages in a format that is pleasing to the eye. A good visualisation strikes a balance between aesthetics and information, where the aesthetic features are designed such that they \textit{"enhance the message of the visualisation"} [@wilke2019]. 

An incorrect balance of aesthetics to information can lead to figures that are misleading, confusing, or unengaging. Wilke discusses the way in which, for example, a research scientist with limited design experience may produce a visualisation displaying the data in an informative manner, but fail to draw immediate attention to the desired message, and on the other hand, someone with a main interest in the aesthetic design of a visualisation could create a figure that is very pleasing to the eye, but create a misleading visualisation in the process.

This literature review will discuss a range of publications discussing various aspects of data visualisation with a focus on how poor or uninformed visualisation design can produce misleading figures, as well as how such visualisations may be abused to deliberately deceive the observer. Starting with publications discussing general good visualisation practice, the discussion will then lead on to look at studies investigating the implementation of different visualisation practices, from which inspiration will be drawn to design the study for this paper.

\section{Good Visualisation Practice}
The book \textit{'Fundamentals of Data Visualization'} [@wilke2019] is renowned as \textit{'an excellent reference about producing and understanding static figures, figures'} [see @wilkerev] and described as being \textit{'suitable to be used as a reference manual'} [see @hwang2020]. Thus, this book provides a good basis to understanding the principles behind data visualisation, and how to create effective, informative and aesthetic figures. 

In the book, Wilke discusses a variety of topics under the data visualisation umbrella, from relatively simple but important and often overlooked ideas such as deciding on coordinate systems, axis scales and colouring, to how to visualise distributions, trends and geospatial data. This literature review will focus on the areas being investigated in the 'Empirical Study of Data Visualisation' survey; namely coordinate systems, axis scaling, colouring for bar charts, alongside stacked and grouped bar charts, as well as axis scaling and formats for time series plots.

\subsection{Axis Scaling and Aspect Ratios}
In discussing coordinate systems and axis scaling, Wilke highlights that, prior to deciding on a coordinate system, it is important to consider the form the data will take, and where the data will be positioned, as well as how many dimensions this data takes. The example used is a classic two-dimensional scatter plot, in which each data value is represented by a point positioned in a distinct location on the 2d plane, and thus two scales are required to define where this location falls, traditionally with a linear scale and horizontal x-axis with the y-axis perpendicular to this. 

Alternative coordinate systems can include the perpendicular model with non-linear axes, or circular or ‘curved’ models such as polar coordinates in addition to flipped axes, where the dependent variable is represented by the x-axis as opposed to the y. 

'An Empirical Study of Data Visualisation' will be mainly analysing perception of categorical bar charts, for which the 'locations' are the category as defined by the x-axis and the bar height. It will be discussed how the perception of these locations could be altered to stray from 'good practice', and how these alterations may mislead the observer. 

In discussion of the linear, two-dimensional cartesian system, the author describes the various formulations that this system can take, in terms of variables with the same or different units. For example, if two variables with different units are represented perpendicular to one another on a cartesian system, the designer has the freedom to stretch or compress the data in a way to best represent the data and, as Wilke states, 'maintain a valid visualisation of the data'. 

Another point of interest mentioned by Wilke here is that the ratio of x to y-axis should be such that 'important differences in position are noticeable'. This is regarded as good practice by Wilke, but could potentially be exploited as discussed by @Few2016; the aspect ratio can be manipulated to make differences appear larger depending on the story that the creator wishes to sell. On the other hand, Wilke does state that it is 'important differences' that should be noticeable, and so may relate to differences that are already significant and crucial to see, and which may be minimised by an inappropriate aspect ratio. 

For example, consider a company facing a drop in profits from one time step to another. An aspect ratio minimising the height in comparison to the width can allow this difference to appear smaller. On the other side of this coin, a company may have marginal profit gain between two time steps, and can abuse principles of perception to lengthen the y-axis as compared to the x, potentially making the difference seem larger.

This will be considered when writing the survey, as the perceived differences in position will be tested when changing features such as y-axis scaling or aspect ratio. A standard practice as laid out by Wilke is that, for two variables with the same unit, the aspect ratio should ensure that the space between ticks for each variable are equal in size, ie. such that the grid lines (real or imagined), form regular squares. This is to ensure that the sizes of tick spacing represent the same values in the same way, as it could be misleading to show two equal numerical differences with different visual spacing. This is regarded as less important, however, for variables in differing units, as the tick spacings represent different values for each variable. Thus, one has less freedom with re-scaling plots while still ensuring an accurate representation of the data when working with variables in the same units. 

The plots in this study will show categorical data, and thus have character variables on the x-axis, with numerical values on the y-axis, and so the effect of altering the aspect ratio on two same-unit variables will not be investigated, but this could be an interesting topic for future investigation.

After this, Wilke goes on to discuss logarithmic scaling, which will be investigated in this study alongside axis truncation. Conversely to what will be investigated in this study, he talks about both logarithmic scaling and log-transformed data whereas this study will consider logarithmic scaling alone. He describes how this is a preferable format when dealing with ratios, and explains that this is a result of the fact that the product of linear numbers is analogous to the sum of the logarithms when using a logarithmic scale. Additionally, data containing a large variation in magnitudes is also stated to benefit from logarithmic scaling.

The book also explains the differences between plotting the original data on a logarithmic scale, and log-transformed data on a linear scale. In terms of mathematics, these are analogous, but Wilke states that plotting the original data on a log-transformed scale is favoured as this shows the true data values as opposed and allows easier reader interpretation of values. This will be considered and the original data on a logarithmic scale will be investigated. A study with narrower scope on only logarithmic scaling and log-transformed data, or even just a study of axis scaling, may allow to test both log-transformations and log-scalings, however the wide scope of this study means that study topics have to be reduced.

\subsection{Colour}
A very important aspect of visualisation is the use of colour. Colour is a very useful means of showing features of the data such as groupings and gradients of values, as well as to highlight key values. It could be said that colour works as a third dimension to the visualisation, showing another dimension of information on top of that shown by the position or size of the data points. It is important that the use of colour is carefully thought out, and not just applied with aesthetics in mind. Aesthetics are, or course, an important factor in colouring a visualisation, as 'pretty' colouring allows the visualisations to be eye-catching and memorable, which can be beneficial to, for example, brands or pharmaceutical companies giving regular data presentations as very aesthetic plots are more likely to be remembered. 

Once again, Wilke has a good explanation and examples of each type of colour usage. There can be defined to be two types of colour scale, or palette; qualitative or quantitative. Qualitative colour scales do not have a logical order of colouring and are used for data for which the ordering of either values of groupings is inconsequential, such as for much categorical data. The latter, quantitative colour scales, provide gradients of colour and can consist of either a gradual scale moving through two or more colours, or a single colour with varying saturation. This type of scale provides a continuous colouring and can show grouping tendencies in continuous data, and is also often used in maps. As mentioned by Wilke, continuous graduated colour scales have the ability to show the degree to which two values are similar. There are several examples of both types of colour scale, and those discussed in this study will involve the defaults for both R and Python, and two colourblind friendly palettes; viridis and greyscale. 

It is important to consider that the visualisations may be viewed by people with colourblindness in order to ensure they are accessible to any viewer. Using certain colour scales may be problematic for someone who is colourblind as they may find it difficult to distinguish between certain colours, and thus lose the impact and story told by the third dimension of the visualisation. @colourb has a good explanation of how visualisation can be modified to accommodate those with the condition. Using colours such as red and green, or a traffic light scaling is not ideal, as these can be harder to distinguish. However, red and green is a very common and useful combination to use, as it can give a highly intuitive story of 'good' and 'bad' values, or positive and negative. Shaffer describes that this can be worked around by adding arrows, icons or annotations to distinguish values. Another workaround is to, for example, use very light green and very dark red as a saturation as opposed to hue comparison. The viridis palette has been specifically formulated to allow easier perception for people with colourblindness.

Colour can very easily be misused, however, and a common misuse, described by Wilke, is to colour each individual bar in a simple bar chart. This colouring reveals no additional information about the data and labeling is much preferred as the added colour here draws attention away from the message of the data and reduces the efficiency of information transmission. However, colour is useful for stacked or grouped bar charts to distinguish between groupings, with each bar in a group relating to a different category, with the labels representing the overall group. 

Additionally, a good way of using colour is to highlight values of interest. For example, one technique is to use a greyscale palette for the majority of the figure, with a small selection of bars or points highlighted for fast communication that these values are most important to the message. This could easily be abused, however, to highlight the required message while potentially hiding or lowering the significance of values that could contradict the message being portrayed. 

In regard to legends, Wilke refers to \textit{'redundant coding'} of legends, which is the principal of using colour as an aesthetic tool to 'enhance' the message of the visualisation as opposed to using this as a primary tool for relaying information. Firstly, as mentioned prior, Wilke discusses how using colour as an identifier can be problematic for people with colourblindness if the colours are poorly selected, and shows how a given colour scheme would look for people with varying forms of colourblindness. The example plots the well-known Iris data set on a scatter plot, separating the species by colour. The colours are poorly chosen, with the colours for two overlapping species becoming almost indistinguishable. Solutions to this are laid out to be changes of colour, or changes of point shape, where the change of point shape provides a fourth perceptual dimension. For line plots this can be seen as dashed or dotted lines.

The author then explains the principle of 'direct labeling', that is, plotting without a legend and rather labeling the objects in the plot itself. This can reduce the amount of information the observer must take in and potentially improve ease of interpretation. Once again, this would be a topic to be investigated further in another study. 

\subsection{Bar Charts}
When discussing good practice for bar charts, Wilke discusses many aspects of visualisation including axis alignment and bar ordering, as well as discussion on stacked and grouped bars. The axis alignment, ie vertical vs horizontal bars, is dependent on the data being visualised. Wilke uses the example of bars with labels that may become either difficult to read or unaesthetic when shown on the vertical chart, but appear clearer on the horizontal. 

In terms of bar ordering, Wilke discusses that it is important aesthetically to order bars from largest to smallest, given there is no pre-specified ordering in the data, such as age ranges. Bar plots will be discussed in more detail in the past study reviews.

\subsection{Visualisation Taxonomy}
The paper @taxonomy provides a 'task by data type' basis for creating visualisations, summarising this with the 'Visual Information Seeking Mantra'; 'Overview first, zoom and filter, then details-on-demand'. This mantra provides a starting point when thinking about creating a visualisation, and relates to the different messages encoded in a visualisation; the viewer must first be able to gain a good overview of the whole data when taking a glance at the plot, but then discern more detail by paying closer attention, as per the 'zoom and filter' principle. The third principle in the mantra is useful to consider when creating interactive visuals; the user is able to, for example, obtain further tables of values and information-based visuals which, and as described in @taylor2014, are 'less visual, and more text-heavy'. The mantra allows the designer to focus on not making visuals too busy whilst also encoding the necessary information. 

Based on this mantra, Shneiderman suggests a 'task by data type taxonomy', which involves cross referencing 7 data types with 7 tasks, for which he doesn't provide a diagram but the description envisions what seems to be a 7-by-7 table of tasks against data types. The idea is to discuss these alongside each other to draw meaningful conclusions as to how best to produce the visual representation.

\section{Studies in Visualisation}
There is a large amount of research and literature surrounding the topic of misleading visualisations, looking into how various techniques can either deliberately or unintentionally deceive an observer in the message of the data. Results from some of these papers will be replicated, as well as used to form hypotheses which this survey will investigate. A large amount of the literature exploring misleading tactics in data visualisation focuses mainly on bar plots and line plots for categorical and time series data, and so this is what the study and literature review will focus on. 

The 2020 paper "The Deceptive Potential of Common Design Tactics Used in Data Visualizations" [@claire-obrian], as the title suggests, explores how using different design tactics may mislead the person seeing the visualisation. Similarly to "An Empirical Study of Data Visualisation", the Claire and O'Brian paper uses a survey to explore how deceptive visualisation techniques can be employed as well as their impact on perception of the data. The survey discussed in this paper presents the participant with four plots; a bar plot, a line plot, a pie chart and a bubble plot. Additionally to changing aesthetic features of the plots themselves, the study investigates the use of exaggerated, leading titles, for example one control plot has the title " Home Sales Show Increase From 2015 - 2016", which is altered to "Huge Increase in Home Sales From 2015 – 2016!". The control plots consist of using a y-axis scaling beginning at 0 for the bar and line plots, a standard pie chart, and a bubble plot with proportionally sized bubbles, all alongside the non-exaggerated titles. The altered plots involve truncating the y-scale for the bar and line plots, making the pie chart in 3D, and arbitrarily altering the sizes of the bubbles on the bubble plot. The altered plots are referred to as the "deceptive" plots. The survey used sets of plots as crossed between deceptive aesthetics and deceptive titles; two had control aesthetics, one with the control title and one for the exaggerated title, and two had deceptive aesthetics with one having the exaggerated titling.

With regard to truncated axes, Claire and O'Brian asked participants to subjectively judge the difference between two data points using a 6 point scale ranging from "a little" to "a lot". For both the bar plot and line plot it was found the the use of a truncated scale increases the perceived difference between the data points. The use of a truncated scale is also discussed by @YANG2021, whereby 5 empirical studies were performed in order to assess the effect of altering the scale in this way. The first of the 5 studies once again assessed how large the difference between data points is perceived to be in the truncated plot as compared to a control, again using a subjective scale from "Not at all different" to "Extremely different" on a 7 point scale. This scale differed, however, in the way that a midpoint label of "Moderately different" was provided. The 7 point scale may be preferable to the 6 point scale as the 7 point has a defined midpoint at 4, whereas the 6 point does not. This study once again concludes that the differences in data points tended to be perceived as larger than for the control plot. Alongside these studies, a 2014 blog post [@parikh_2014] discusses axis truncation and its effect on perceived data point difference for bar plots alongside other aesthetic features. The first example shows how truncating the y-axis of a bar plot can over-exaggerate differences in the heights of the bars, perhaps leading to incorrect observations regarding comparisons of values within the data. 

The paper @stackscale performs a similar study, but instead investigates the use of 'stack-scale', or 'stacked' bar charts and logarithmic scaling. The aim of the study was to explore whether stack-scale bar charts are an effective way to visualise large value data, which is less relevant to since the Ninja Warrior and Sales data are relatively low-valued data compared to the paper, but nevertheless provides a framework for exploring the use of logarithmic scaling and stacked bars in a respondent study. Participants were shown three plots; a control with a linear scale, a bar plot using a stack-scale, and one with logarithmic scaling. The questions asked determined how the different scaling affected accuracy in reading individual values, interpreting differences in values and determining which time-step exhibits the largest difference in values. @logax additionally discusses the use of a logarithmic axis in bar plots, explaining how it is impossible for a zero value to be displayed on this axis, and thus the bar start points are arbitrary and produce an inaccurate representation of the bar height with relation to the true value. To quote the paper, \textit{"Don’t create bar graphs using a logarithmic axis if your goal is to honestly show the data"}. It can be observed that the logarithmic scale makes the perceived difference appear smaller than in the control. 

As well as scaling, another aspect of visualisation design that could potentially mislead the observer is bar width and aspect ratios. When adding a visualisation into a publication, re-sizing the visualisation to fit a specific gap may include altering the aspect ratio, in turn affecting the length to width ratio of the bars in a bar plot. As explored by Steven Few in a 2016 article for the \textit{'Visual Business Intelligence Newsletter'} [@Few2016], altering this ratio can affect viewer perception in the way of a narrower and taller image distorting bars to appear longer, and vice versa, meaning that perceived differences between bar heights may be affected. 

Part 2 of the survey will be based around investigating this idea, alongside how the reading of exact values is affected. The second section of the survey tests whether altering aspect ratio of plots affects interpretation. The purpose of this is to mirror what my occur when visualisations are published, and may be resized to fit the section of the page they sit on. As in [@Few2016], it will be hypothesised that an aspect ratio that effectively narrows the bars may cause overestimation in values, and vice versa, using a ratio that widens bars could lead to underestimation. In the paper, the author discusses how increasing the widths of bars could distract from the bar height as well as take up excessive space on a page. It is also mentioned that wider bars may be "aesthetically displeasing". This section of the survey will test both how bar width alters perceived difference between bars as well as opinions on the aesthetics. The method in the paper also involves altering spaces between bars, including bar plots with spaces at 50% of the bar widths and then reducing the width of the space by a third. Conversely to this, width of spaces between bars will not be considered, only the effective widths of the bars themselves. The author concludes that a length-to-width ratio of 10:1 appears to suffer from perceptual imbalance, but increasing this such that the bars become narrower and longer does not appear to have as much of an impact; the ratio can be increased relatively far with out causing much perceptual imbalance.


An article from the University of Stuttgart [@HuynhHaiDang2017] gives an overview of many types of bar chart, including stacked and grouped bars. The author remarks that grouped bar charts may make the comparison of bars in the same category more difficult, while the stacked bar chart sacrifices ease of comparison of values in the bars for increased spacial efficiency. A 2018 work from the journal of \textit{'Visual Informatics'} [@INDRATMO2018155] also provides a discussion on the use of various forms of stacked and grouped bar charts and their efficacy. The paper notes how a classical stacked bar chart can be useful for overall comparisons as the height of the bar represents the value of the item, with the different attributes depicted as a segmentation of this single bar into different colours. When discussing grouped bar charts it is mentioned that stacked bar charts may be less useful when performing attribute comparisons, in other words comparisons between different categories on the same bar, as a result of the bar segments being non-aligned. This results in comparison taking the form of length judgment as opposed to position judgment. Cleveland and McGill in their 1984 article in the \textit{'Journal of the American Statistical Association'} [@clevelandmcgill] discuss how judgments based on length are likely to be less accurate than those based on position. A grouped bar chart is a way to allow for easy comparison between individual categories, but is discussed to be less effective in overall comparison. 

\section{Coding of Visualisations}

\subsection{Choosing the correct tool}
In @wilke2019, the author discusses how to choose the right visualisation software, exploring the areas of 'Reproducibility and Repeatability', 'Data exploration versus data presentation', and 'Separation of content and design'. 

He starts off by remarking that a person will tend to lean into producing visuals in their already preferred tools, and he mentions the 'strong emotional bond' one may have to their preferred tool. This is discussed as a not necessarily a negative concept, since the process of training to use a new tool can be time-consuming and mentally taxing, with the end thus results potentially not being as accomplished or polished as if the individual had spent the same time and effort creating the visual with the tool they know. 

This factor may be assessed on a case-by-case basis; if the preferred tool provides adequate results then this may be favoured as more time and energy can be put into creating the graphic itself as opposed to learning a new tool, however if another tool performs the given task remarkably better than the preferred, then it should be at least considered in the visual design process. This will be investigated in the interviews, as there is roughly a 50/50 split between respondents preferring R, and those favouring Python, and so it will be interesting to see if this impacts opinions on the tool used to create the visuals.

The author states that \textit{'The best visualization software is the one that allows you to make the figures you need'}, meaning that if the old tool allows a user to produce visuals better than leaning a new one, even though the new may be better suited, it would be preferable to choose whichever allows the needed figures to be created for the individual. However, he then goes on to explain the benefits of having the knowledge of many tools in an arsenal, as this can allow the user to make an informed decision on the best tool for the job, much like an engineer with a tool box. 

The definitions of 'reproducability' and 'repeatability' used by Wilke closely mirror those used in investigative science; 'reproducability' is the ability of the visualisation to be re-created by another user with access to the data, and 'repeatabiity' is the ability of the visualisation to be exactly replicated by the original creator. It is important to note here that Wilke remarks that a 'reproducible' visualisation does not need to be identical to the original in terms of aesthetics, but the content of the plot should be the same. Wilke does, however, state that the 'repeatable' graphics should be able to be identical every time they are created. 

\subsection{Interactive visualisation tools}
One conclusion Wilke draws is that less programmatic and more interactive tools, for example Tableau [@tableau] or Spotfire [@spotfire], may reduce the reproducibility of visualisations as a result of the live visualisation editing with more limited active change tracking. The lack of change tracking can lead the original creator of a visual to forget what alterations have been made between a start and finish product if a manual effort to track changes is not made, resulting in neither the original creator nor another individual being able to replicate or reproduce the visualisation easily. The author even states that he tries to \textit{'stay away from interactive programs as much as possible'}, as programmatic methods, for which data and scripts may be easily shared, lend themselves much more to reproduction and replication. 

There are benefits to interactive plotting tools, however, as discussed by @LUO2019113061. Such tools are beneficial for presenting data in a way that allows audience questions or queries regarding the data to be investigated and answered in real time, as opposed to programmatic methods, which are fairly fixed during presentation, with modifications involving altering and re-running the underlying code. Programmatic methods, in this way, can be more time consuming and less flexible, and also have a strength in automation.

A tool of an interactive nature allows the user to closely follow the aforementioned \textit{'Visual Information Seeking Mantra'}, as they are able to actively control the depth and complexity of information being presented, once again in real time. As well as for presentations, an interactive format is useful for data exploration, perhaps for discovering patterns and connections as a precursor to creating similar visuals using a programming tool. Interactive tools allow the user to filter, sort and really drill down into the data with a few clicks, as opposed to having to re-run many lines of code every time an alteration is made.

Whether to choose an interactive tool or not depends heavily on the goal of the visualisation, and the medium with which it is being presented. For example, static, programmatic visuals, would be ideal for a scientific research paper, but a pharmaceutical representative presenting study data to healthcare professionals may benefit from the abilities of an interactive tool. 

The JavaScript library d3.js [@d3] is an example of a tool that, while lacking in its ability to clean and manipulate data, can take advantage of both programmatic methods and interactivity.This is explained by @javapy, where the author also discusses how the programmatic nature of Python can be utilised in conjunction with the ability of JavaScript to create dynamic, browser-based visuals. In the book, it is explained how the early stages of data cleaning, manipulation and exploration can be done in Python, with the transformation into interactive visual information being undertaken by D3, as per figure \ref{toolchain}, presented in the book. This process is referred to as the 'dataviz toolchain'.

D3 may also be used alone to create visuals, however employing Python in the early stages allows for easier data handling and manipulation. Prior to employing D3, interactive Python interfaces such as \textit{IPython} can be used to explore the data with packages such as \textit{Seaborn} or \textit{Matplotlib}, allowing for cleaning and refinement of data for visualisation in D3.

D3 itself makes use of Cascading Style Sheets (CSS) and Scalar Vector Graphics (SVG) to make high quality visuals. The use of SVG is particularly beneficial as these graphics are able to be re-scaled to any dimension without becoming blurry or distorted, meaning they do not suffer from a loss of quality when being scaled. This is a strength of D3 for visualisation, in addition to the interactive and dynamic visuals.

![The Dataviz Toolchain \label{toolchain}](C:/Users/Katie/OneDrive/Uni_Work_Year4/project/Year-4-Project/markdown/chapters/Interviews/toolchain.jpg)

When it comes to static visuals, as investigated in this study, two commonly used tools as the \textit{ggplot2} package in R and the \textit{matplotlib} library in Python, which are investigated here. 

\section{Background on R and Python}
R and Python are two languages that differ in programming paradigms, with R being a mostly functional language and Python taking more of a multi-paradigm approach, incorporating mainly elements of object-oriented programming, but along with some elements of procedure-oriented and functional paradigms [@paradigm]. 

At a basic level, functional programming does exactly what the name says; it operates mainly using functions to obtain results and output, with the ability to use certain functions within others to create new composite functions.

Object-oriented programming, on the other hand, focuses on storing information in 'objects', with attributes as defined by their 'class'. Information in a class structure is then accessed in the format 'class.object'. In addition to objects, 'methods', or functions, can be defined within a class.

These two differing methods of programming, like everything, have a series of positives and negatives. A positive shared by both R and Python is that they are open-source, unlike languages such as SAS or MATLAB, meaning anyone can gain access with no paywalls, and anyone can contribute to development of new packages and libraries, creating communities among people who develop in these languages and allowing creativity of people from all areas to be used in development. Both of these are also fairly good for creating visualisations in a reproducable and replicable way due to their programmatic nature.

Some differences between the two languages are outlined by @rvpy. One main difference is R's main purpose of data exploration and statistical analysis, as compared to Python use in more general programming, but with applications in data wrangling. Both therefore can be used for data cleaning, analysis and visualisation, but R is more suited than Python to running in depth statistical testing and analysis, whereas Python is more suited than R to applications such as large-scale machine learning. 

These differences are reflected in the visualisation capabilities of each language; with a focus in data exploration, R comes with the pre-installed plotting library \textit{graphics}, whereas the more general programming-based Python must use external libraries. Additionally to the default library, R can also install \textit{'ggplot2'}, a library dedicated to producing elegant visuals with ease.

Python is regarded as a fairly easy to learn language and is known as one of the most popular languages worldwide, especially among programmers and developers. R appears to be noted as a generally slightly more difficult to learn language, but is a popular choice among statisticians and data scientists, particularly in R&D.

\subsection{The Grammar of Graphics}
First introduced by Leland Wilkinson in his 1999 book "The Grammar of Graphics" [@wilkinson2009], the 'grammar of graphics' is explained by Hadley Wickham in the 2010 paper \textit{A Layered Grammar of Graphics} [@layered-grammar] as a method to describe the underlying components of a visualisation. 

It works on breaking every plot into a layers of graphic components, building visualisations in a systematic way from the ground up. The first layer consists of the data and a basic aesthetic mapping, with subsequent layers being made up of components such as scales, coordinate systems, positioning and geometric objects. Each component, or layer, of a layered graphic can be considered as independent in this framework, with each being able to be modified without altering any other component.

This concept allows effective visualisations to be built in an efficient way, ensuring all important aspects of a visualisation are considered and accounted for, as opposed to randomly assigning aesthetics to a figure on a 'trial and error' basis. It allows for the creation of figures that are bespoke to each problem, with every small element of a plot being fully under control of the designer.

The transition to learning a grammatical programming system such as \textit{ggplot} or \textit{matplotlib} from using a software using a stock library of graphics with tweak-able elements is described by the author as akin to transitioning to learning \latex after using MS Word. His reasoning is that it encompasses the initial frustration and feeling of lack of control over 'low-level aspects', but the eventual freedom of being more deep content focused, leading in the case of plots to creating 'richer graphics much more easily'. 

\subsection{Visualising with \textit{ggplot2}}
As mentioned prior \textit{ggplot2} is an R package dedicated to providing an easy way to produce elegant but complex visualisations, with applications in simple to complex data analysis and statistics along with areas such as machine learning. 

This package is built to operate in a way that strongly follows the the grammar of graphics [@ggplot2], perhaps expected given that the author of \textit{A Layered Grammar of Graphics} is Harvey Wickham, referenced prior as the creator of this package. This can be clearly seen in the structure of commands for creating plots in this package; the first step in creating a plot in this package is to specify a data frame and specify an aesthetic mapping using the `aes()` argument. 
 
After these have been specified, the user ends the line with a `+`, which dictates the addition of the next line, which will comprise of one of the higher level layers to the base layer aesthetic mapping. It can take some time to learn this format, but becomes intuitive once the user has an understanding of the concept of the grammar of graphics, even if they have not explicitly learned it or heard of this term, and rather have picked it up through practice.

\textit{ggplot2} has the capacity to create complex visuals while staying informative and retaining aesthetic appeal, and in the right hands can be an incredibly powerful tool for visualisation of data. 

\subsection{Visualising with \textit{matplotlib}}
\textit{Matplotlib} is a Python plotting library that allows the user to create MATLAB-like visuals while being free and open-source.

There exist two Application programming interfaces (APIs) for the creation of plots in \textit{matplotlib}; the object oriented interface of base \textit{matplotlib} and the more functional \textit{pyplot}. Explained by @matplotblog, both interfaces seem to provide effective ways to create elegant and informative visuals, with the main differences being that \textit{pyplot} structures the building of visuals in a very similar way to MATLAB in terms of syntax and methodology.

Both APIs follow the grammar of graphics, with the initial step in visualisation being the creation of a blank plot to which various graphic features can be added sequentially. 

This method of plotting, like R, provides a very good way to produce visuals, however unlike R does not specialise in visualisation and relies on imports and collaborations with other languages, such as MATLAB and JavaScript. The combination of Python with these other languages, however, provides very effective methods of producing compelling visuals, and again can be a very powerful tool.





