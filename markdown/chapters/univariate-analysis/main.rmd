---
output:
  pdf_document: 
    extra_dependencies: ["float"]
fontsize: 12pt
geometry: margin=1in
documentclass: book
header-includes:
  \setlength{\parskip}{1em}
classoption:
- twocolumn
---

```{r setup, include=FALSE, warning=F}
knitr::opts_chunk$set(echo = F, warning = F, comment = NA, fig.pos = "!H")
```

```{r Data manipulation, include = FALSE}
rm(list=ls())
library(readxl)
library(dplyr)
library(ggplot2)
library(rstatix)
library(BSDA)
library(gridExtra)
library(kableExtra)
library(lawstat)

require(pander)
panderOptions('round', 2)


###### SET UP DATA ######
for(i in 1:6){
  r <- assign(paste0("r",i), (as.data.frame(read_excel("C:/Users/Katie/OneDrive/Uni_Work_Year4/Project/Year-4-Project/survey responses/raw.xlsx", paste0("R - v",i)))))
  py <- assign(paste0("py",i), (as.data.frame(read_excel("C:/Users/Katie/OneDrive/Uni_Work_Year4/Project/Year-4-Project/survey responses/raw.xlsx", paste0("Py - v",i)))))
  assign(paste0("ver",i), rbind(r, py))
}

ver1 <- ver1[-1,]
r1 <- r1[-1,]

names(ver1) <- c("timestamp", "consent", "age", "uni", "sp_aware", "obs_skl", "num_skl", "cblind", "vis_pro",
                 
                 "con_1", "con_2", "con_3", "con_4", 
                 "log_1", "log_2", "log_3", "log_4",
                 "trn_1", "trn_2", "trn_3", "trn_4",
                 
                 "def_1", "def_2", "def_3", 
                 "nar_1", "nar_2", "nar_3", 
                 "wid_1", "wid_2", "wid_3",
                 "all_1", "all_2", "all_3",
                 
                 "vir_sta_1", "vir_sta_2", "vir_sta_3", "vir_sta_4", 
                 "vir_grp_1", "vir_grp_2", "vir_grp_3", "vir_grp_4",
                 "sta_grp",
                 "a_cols_1", "a_cols_2", 
                 
                 "sep_1a", "sep_1b", "sep_2",
                 "ab_trn_1a", "ab_trn_1b", "ab_trn_2", 
                 "ab_zro_1", "ab_zro_1b", "ab_zro_2",
                 
                 "cd_trn",
                 "cd_zro"
)

names(r1) <- c("timestamp", "consent", "age", "uni", "sp_aware", "obs_skl", "num_skl", "cblind", "vis_pro",
               
               "con_1", "con_2", "con_3", "con_4", 
               "log_1", "log_2", "log_3", "log_4",
               "trn_1", "trn_2", "trn_3", "trn_4",
               
               "def_1", "def_2", "def_3", 
               "nar_1", "nar_2", "nar_3", 
               "wid_1", "wid_2", "wid_3",
               "all_1", "all_2", "all_3",
               
               "vir_sta_1", "vir_sta_2", "vir_sta_3", "vir_sta_4", 
               "vir_grp_1", "vir_grp_2", "vir_grp_3", "vir_grp_4",
               "sta_grp",
               "a_cols_1", "a_cols_2", 
               
               "sep_1a", "sep_1b", "sep_2",
               "ab_trn_1a", "ab_trn_1b", "ab_trn_2", 
               "ab_zro_1", "ab_zro_1b", "ab_zro_2",
               
               "cd_trn",
               "cd_zro"
)

names(py1) <- c("timestamp", "consent", "age", "uni", "sp_aware", "obs_skl", "num_skl", "cblind", "vis_pro",
                
                "con_1", "con_2", "con_3", "con_4", 
                "log_1", "log_2", "log_3", "log_4",
                "trn_1", "trn_2", "trn_3", "trn_4",
                
                "def_1", "def_2", "def_3", 
                "nar_1", "nar_2", "nar_3", 
                "wid_1", "wid_2", "wid_3",
                "all_1", "all_2", "all_3",
                
                "vir_sta_1", "vir_sta_2", "vir_sta_3", "vir_sta_4", 
                "vir_grp_1", "vir_grp_2", "vir_grp_3", "vir_grp_4",
                "sta_grp",
                "a_cols_1", "a_cols_2", 
                
                "sep_1a", "sep_1b", "sep_2",
                "ab_trn_1a", "ab_trn_1b", "ab_trn_2", 
                "ab_zro_1", "ab_zro_1b", "ab_zro_2",
                
                "cd_trn",
                "cd_zro")


names(ver2) <- c("timestamp", "consent", "age", "uni", "sp_aware", "obs_skl", "num_skl", "cblind", "vis_pro",
                 
                 "con_1", "con_2", "con_3", "con_4", 
                 "trn_1", "trn_2", "trn_3", "trn_4",
                 "log_1", "log_2", "log_3", "log_4",
                 
                 "def_1", "def_2", "def_3", 
                 "wid_1", "wid_2", "wid_3",
                 "nar_1", "nar_2", "nar_3", 
                 "all_1", "all_2", "all_3",
                 
                 "def_sta_1", "def_sta_2", "def_sta_3", "def_sta_4", 
                 "def_grp_1", "def_grp_2", "def_grp_3", "def_grp_4",
                 "sta_grp",
                 "b_cols_1", "b_cols_2", 
                 
                 "sep_1a", "sep_1b", "sep_2",
                 "ab_zro_1", "ab_zro_1b", "ab_zro_2",
                 "ab_trn_1a", "ab_trn_1b", "ab_trn_2", 
                 
                 "cd_trn",
                 "cd_zro"
)

names(r2) <- c("timestamp", "consent", "age", "uni", "sp_aware", "obs_skl", "num_skl", "cblind", "vis_pro",
               
               "con_1", "con_2", "con_3", "con_4", 
               "trn_1", "trn_2", "trn_3", "trn_4",
               "log_1", "log_2", "log_3", "log_4",
               
               "def_1", "def_2", "def_3", 
               "wid_1", "wid_2", "wid_3",
               "nar_1", "nar_2", "nar_3", 
               "all_1", "all_2", "all_3",
               
               "def_sta_1", "def_sta_2", "def_sta_3", "def_sta_4", 
               "def_grp_1", "def_grp_2", "def_grp_3", "def_grp_4",
               "sta_grp",
               "b_cols_1", "b_cols_2", 
               
               "sep_1a", "sep_1b", "sep_2",
               "ab_zro_1", "ab_zro_1b", "ab_zro_2",
               "ab_trn_1a", "ab_trn_1b", "ab_trn_2", 
               
               "cd_trn",
               "cd_zro"
)

names(py2) <- c("timestamp", "consent", "age", "uni", "sp_aware", "obs_skl", "num_skl", "cblind", "vis_pro",
                
                "con_1", "con_2", "con_3", "con_4", 
                "trn_1", "trn_2", "trn_3", "trn_4",
                "log_1", "log_2", "log_3", "log_4",
                
                "def_1", "def_2", "def_3", 
                "wid_1", "wid_2", "wid_3",
                "nar_1", "nar_2", "nar_3", 
                "all_1", "all_2", "all_3",
                
                "def_sta_1", "def_sta_2", "def_sta_3", "def_sta_4", 
                "def_grp_1", "def_grp_2", "def_grp_3", "def_grp_4",
                "sta_grp",
                "b_cols_1", "b_cols_2", 
                
                "sep_1a", "sep_1b", "sep_2",
                "ab_zro_1", "ab_zro_1b", "ab_zro_2",
                "ab_trn_1a", "ab_trn_1b", "ab_trn_2", 
                
                "cd_trn",
                "cd_zro"
)


names(ver3) <- c("timestamp", "consent", "age", "uni", "sp_aware", "obs_skl", "num_skl", "cblind", "vis_pro",
                 
                 "log_1", "log_2", "log_3", "log_4",
                 "con_1", "con_2", "con_3", "con_4", 
                 "trn_1", "trn_2", "trn_3", "trn_4",
                 
                 "nar_1", "nar_2", "nar_3", 
                 "def_1", "def_2", "def_3", 
                 "wid_1", "wid_2", "wid_3",  
                 "all_1", "all_2", "all_3",
                 
                 "def_grp_1", "def_grp_2", "def_grp_3", "def_grp_4",
                 "def_sta_1", "def_sta_2", "def_sta_3", "def_sta_4", 
                 "sta_grp",
                 "c_cols_1", "c_cols_2", 
                 
                 "ab_trn_1a", "ab_trn_1b", "ab_trn_2", 
                 "sep_1a", "sep_1b", "sep_2",
                 "ab_zro_1", "ab_zro_1b", "ab_zro_2",                
                 
                 "cd_zro",
                 "cd_trn"
)

names(r3) <- c("timestamp", "consent", "age", "uni", "sp_aware", "obs_skl", "num_skl", "cblind", "vis_pro",
               
               "log_1", "log_2", "log_3", "log_4",
               "con_1", "con_2", "con_3", "con_4", 
               "trn_1", "trn_2", "trn_3", "trn_4",
               
               "nar_1", "nar_2", "nar_3", 
               "def_1", "def_2", "def_3", 
               "wid_1", "wid_2", "wid_3",  
               "all_1", "all_2", "all_3",
               
               "def_grp_1", "def_grp_2", "def_grp_3", "def_grp_4",
               "def_sta_1", "def_sta_2", "def_sta_3", "def_sta_4", 
               "sta_grp",
               "c_cols_1", "c_cols_2", 
               
               "ab_trn_1a", "ab_trn_1b", "ab_trn_2", 
               "sep_1a", "sep_1b", "sep_2",
               "ab_zro_1", "ab_zro_1b", "ab_zro_2",                
               
               "cd_zro",
               "cd_trn"
)

names(py3) <- c("timestamp", "consent", "age", "uni", "sp_aware", "obs_skl", "num_skl", "cblind", "vis_pro", 
                "log_1", "log_2", "log_3", "log_4",
                "con_1", "con_2", "con_3", "con_4", 
                "trn_1", "trn_2", "trn_3", "trn_4",
                
                "nar_1", "nar_2", "nar_3", 
                "def_1", "def_2", "def_3", 
                "wid_1", "wid_2", "wid_3",  
                "all_1", "all_2", "all_3",
                
                "def_grp_1", "def_grp_2", "def_grp_3", "def_grp_4",
                "def_sta_1", "def_sta_2", "def_sta_3", "def_sta_4", 
                "sta_grp",
                "c_cols_1", "c_cols_2", 
                
                "ab_trn_1a", "ab_trn_1b", "ab_trn_2", 
                "sep_1a", "sep_1b", "sep_2",
                "ab_zro_1", "ab_zro_1b", "ab_zro_2",                
                
                "cd_zro",
                "cd_trn"
)


names(ver4) <- c("timestamp", "consent", "age", "uni", "sp_aware", "obs_skl", "num_skl", "cblind", "vis_pro",
                 
                 "log_1", "log_2", "log_3", "log_4",
                 "trn_1", "trn_2", "trn_3", "trn_4",
                 "con_1", "con_2", "con_3", "con_4", 
                 
                 "nar_1", "nar_2", "nar_3", 
                 "wid_1", "wid_2", "wid_3",
                 "def_1", "def_2", "def_3", 
                 "all_1", "all_2", "all_3",
                 
                 "gr_sta_1", "gr_sta_2", "gr_sta_3", "gr_sta_4",
                 "gr_grp_1", "gr_grp_2", "gr_grp_3", "gr_grp_4",  
                 "sta_grp",
                 "d_cols_1", "d_cols_2", 
                 
                 "ab_trn_1a", "ab_trn_1b", "ab_trn_2", 
                 "ab_zro_1", "ab_zro_1b", "ab_zro_2",
                 "sep_1a", "sep_1b", "sep_2",
                 
                 "cd_trn",
                 "cd_zro"
)

names(r4) <- c("timestamp", "consent", "age", "uni", "sp_aware", "obs_skl", "num_skl", "cblind", "vis_pro",
               
               "log_1", "log_2", "log_3", "log_4",
               "trn_1", "trn_2", "trn_3", "trn_4",
               "con_1", "con_2", "con_3", "con_4", 
               
               "nar_1", "nar_2", "nar_3", 
               "wid_1", "wid_2", "wid_3",
               "def_1", "def_2", "def_3", 
               "all_1", "all_2", "all_3",
               
               "gr_sta_1", "gr_sta_2", "gr_sta_3", "gr_sta_4",
               "gr_grp_1", "gr_grp_2", "gr_grp_3", "gr_grp_4",  
               "sta_grp",
               "d_cols_1", "d_cols_2", 
               
               "ab_trn_1a", "ab_trn_1b", "ab_trn_2", 
               "ab_zro_1", "ab_zro_1b", "ab_zro_2",
               "sep_1a", "sep_1b", "sep_2",
               
               "cd_trn",
               "cd_zro"
)

names(py4) <- c("timestamp", "consent", "age", "uni", "sp_aware", "obs_skl", "num_skl", "cblind", "vis_pro",
                
                "log_1", "log_2", "log_3", "log_4",
                "trn_1", "trn_2", "trn_3", "trn_4",
                "con_1", "con_2", "con_3", "con_4", 
                
                "nar_1", "nar_2", "nar_3", 
                "wid_1", "wid_2", "wid_3",
                "def_1", "def_2", "def_3", 
                "all_1", "all_2", "all_3",
                
                "gr_sta_1", "gr_sta_2", "gr_sta_3", "gr_sta_4",
                "gr_grp_1", "gr_grp_2", "gr_grp_3", "gr_grp_4",  
                "sta_grp",
                "d_cols_1", "d_cols_2", 
                
                "ab_trn_1a", "ab_trn_1b", "ab_trn_2", 
                "ab_zro_1", "ab_zro_1b", "ab_zro_2",
                "sep_1a", "sep_1b", "sep_2",
                
                "cd_trn",
                "cd_zro"
)


names(ver5) <- c("timestamp", "consent", "age", "uni", "sp_aware", "obs_skl", "num_skl", "cblind", "vis_pro",
                 
                 "trn_1", "trn_2", "trn_3", "trn_4",
                 "con_1", "con_2", "con_3", "con_4",
                 "log_1", "log_2", "log_3", "log_4", 
                 
                 "wid_1", "wid_2", "wid_3",
                 "def_1", "def_2", "def_3", 
                 "nar_1", "nar_2", "nar_3", 
                 "all_1", "all_2", "all_3",
                 
                 "vir_grp_1", "vir_grp_2", "vir_grp_3", "vir_grp_4",
                 "vir_sta_1", "vir_sta_2", "vir_sta_3", "vir_sta_4", 
                 "sta_grp",
                 "e_cols_1", "e_cols_2", 
                 
                 "ab_zro_1", "ab_zro_1b", "ab_zro_2",
                 "sep_1a", "sep_1b", "sep_2",
                 "ab_trn_1a", "ab_trn_1b", "ab_trn_2", 
                 
                 "cd_zro",
                 "cd_trn"
)

names(r5) <- c("timestamp", "consent", "age", "uni", "sp_aware", "obs_skl", "num_skl", "cblind", "vis_pro",
               
               "trn_1", "trn_2", "trn_3", "trn_4",
               "con_1", "con_2", "con_3", "con_4",
               "log_1", "log_2", "log_3", "log_4", 
               
               "wid_1", "wid_2", "wid_3",
               "def_1", "def_2", "def_3", 
               "nar_1", "nar_2", "nar_3", 
               "all_1", "all_2", "all_3",
               
               "vir_grp_1", "vir_grp_2", "vir_grp_3", "vir_grp_4",
               "vir_sta_1", "vir_sta_2", "vir_sta_3", "vir_sta_4", 
               "sta_grp",
               "e_cols_1", "e_cols_2", 
               
               "ab_zro_1", "ab_zro_1b", "ab_zro_2",
               "sep_1a", "sep_1b", "sep_2",
               "ab_trn_1a", "ab_trn_1b", "ab_trn_2", 
               
               "cd_zro",
               "cd_trn"
)

names(py5) <- c("timestamp", "consent", "age", "uni", "sp_aware", "obs_skl", "num_skl", "cblind", "vis_pro",
                
                "trn_1", "trn_2", "trn_3", "trn_4",
                "con_1", "con_2", "con_3", "con_4",
                "log_1", "log_2", "log_3", "log_4", 
                
                "wid_1", "wid_2", "wid_3",
                "def_1", "def_2", "def_3", 
                "nar_1", "nar_2", "nar_3", 
                "all_1", "all_2", "all_3",
                
                "vir_grp_1", "vir_grp_2", "vir_grp_3", "vir_grp_4",
                "vir_sta_1", "vir_sta_2", "vir_sta_3", "vir_sta_4", 
                "sta_grp",
                "e_cols_1", "e_cols_2", 
                
                "ab_zro_1", "ab_zro_1b", "ab_zro_2",
                "sep_1a", "sep_1b", "sep_2",
                "ab_trn_1a", "ab_trn_1b", "ab_trn_2", 
                
                "cd_zro",
                "cd_trn"
)

names(ver6) <- c("timestamp", "consent", "age", "uni", "sp_aware", "obs_skl", "num_skl", "cblind", "vis_pro",
                 
                 "trn_1", "trn_2", "trn_3", "trn_4",
                 "log_1", "log_2", "log_3", "log_4", 
                 "con_1", "con_2", "con_3", "con_4",
                 
                 "wid_1", "wid_2", "wid_3",
                 "nar_1", "nar_2", "nar_3", 
                 "def_1", "def_2", "def_3", 
                 "all_1", "all_2", "all_3",
                 
                 "gr_grp_1", "gr_grp_2", "gr_grp_3", "gr_grp_4",
                 "gr_sta_1", "gr_sta_2", "gr_sta_3", "gr_sta_4", 
                 "sta_grp",
                 "f_cols_1", "f_cols_2", 
                 
                 "ab_zro_1", "ab_zro_1b", "ab_zro_2",
                 "ab_trn_1a", "ab_trn_1b", "ab_trn_2", 
                 "sep_1a", "sep_1b", "sep_2",
                 
                 "cd_zro",
                 "cd_trn"
)

names(r6) <- c("timestamp", "consent", "age", "uni", "sp_aware", "obs_skl", "num_skl", "cblind", "vis_pro",
               
               "trn_1", "trn_2", "trn_3", "trn_4",
               "log_1", "log_2", "log_3", "log_4", 
               "con_1", "con_2", "con_3", "con_4",
               
               "wid_1", "wid_2", "wid_3",
               "nar_1", "nar_2", "nar_3", 
               "def_1", "def_2", "def_3", 
               "all_1", "all_2", "all_3",
               
               "gr_grp_1", "gr_grp_2", "gr_grp_3", "gr_grp_4",
               "gr_sta_1", "gr_sta_2", "gr_sta_3", "gr_sta_4", 
               "sta_grp",
               "f_cols_1", "f_cols_2", 
               
               "ab_zro_1", "ab_zro_1b", "ab_zro_2",
               "ab_trn_1a", "ab_trn_1b", "ab_trn_2", 
               "sep_1a", "sep_1b", "sep_2",
               
               "cd_zro",
               "cd_trn"
)

names(py6) <- c("timestamp", "consent", "age", "uni", "sp_aware", "obs_skl", "num_skl", "cblind", "vis_pro",
                
                "trn_1", "trn_2", "trn_3", "trn_4",
                "log_1", "log_2", "log_3", "log_4", 
                "con_1", "con_2", "con_3", "con_4",
                
                "wid_1", "wid_2", "wid_3",
                "nar_1", "nar_2", "nar_3", 
                "def_1", "def_2", "def_3", 
                "all_1", "all_2", "all_3",
                
                "gr_grp_1", "gr_grp_2", "gr_grp_3", "gr_grp_4",
                "gr_sta_1", "gr_sta_2", "gr_sta_3", "gr_sta_4", 
                "sta_grp",
                "f_cols_1", "f_cols_2", 
                
                "ab_zro_1", "ab_zro_1b", "ab_zro_2",
                "ab_trn_1a", "ab_trn_1b", "ab_trn_2", 
                "sep_1a", "sep_1b", "sep_2",
                
                "cd_zro",
                "cd_trn"
)

ver2$uni[which(is.na(ver2$uni))] <- "None"

###### Y SCALES ######
ctrl_y_scale <- rbind(ver1[,c(4:7, 10:13)], ver2[,c(4:7, 10:13)], ver3[,c(4:7, 14:17)], ver4[,c(4:7, 18:21)], ver5[,c(4:7, 14:17)], ver6[,c(4:7, 18:21)])
log_y_scale <- rbind(ver1[,c(4:7, 14:17)], ver2[,c(4:7, 18:21)], ver3[,c(4:7, 10:13)], ver4[,c(4:7, 10:13)], ver5[,c(4:7, 18:21)], ver6[,c(4:7, 14:17)])
trnc_y_scale <- rbind(ver1[,c(4:7, 18:21)], ver2[,c(4:7, 14:17)], ver3[,c(4:7, 18:21)], ver4[,c(4:7, 14:17)], ver5[,c(4:7, 10:13)], ver6[,c(4:7, 10:13)])

ctrl_y_scale_r <- rbind(r1[,c(4:7, 10:13)], r2[,c(4:7, 10:13)], r3[,c(4:7, 14:17)], r4[,c(4:7, 18:21)], r5[,c(4:7, 14:17)], r6[,c(4:7, 18:21)])
log_y_scale_r<- rbind(r1[,c(4:7, 14:17)], r2[,c(4:7, 18:21)], r3[,c(4:7, 10:13)], r4[,c(4:7, 10:13)], r5[,c(4:7, 18:21)], r6[,c(4:7, 14:17)])
trnc_y_scale_r <- rbind(r1[,c(4:7, 18:21)], r2[,c(4:7, 14:17)], r3[,c(4:7, 18:21)], r4[,c(4:7, 14:17)], r5[,c(4:7, 10:13)], r6[,c(4:7, 10:13)])

ctrl_y_scale_py <- rbind(py1[,c(4:7, 10:13)], py2[,c(4:7, 10:13)], py3[,c(4:7, 14:17)], py4[,c(4:7, 18:21)], py5[,c(4:7, 14:17)], py6[,c(4:7, 18:21)])

log_y_scale_py <- rbind(py1[,c(4:7, 14:17)], py2[,c(4:7, 18:21)], py3[,c(4:7, 10:13)], py4[,c(4:7, 10:13)], py5[,c(4:7, 18:21)], py6[,c(4:7, 14:17)])

trnc_y_scale_py <- rbind(py1[,c(4:7, 18:21)], py2[,c(4:7, 14:17)], py3[,c(4:7, 18:21)], py4[,c(4:7, 14:17)], py5[,c(4:7, 10:13)], py6[,c(4:7, 10:13)])

for(i in c(unique(ctrl_y_scale$uni), unique(trnc_y_scale$uni), unique(log_y_scale$uni))){
  assign(paste0(i, "_ctrl"), ctrl_y_scale[which(ctrl_y_scale$uni==i),])
  assign(paste0(i, "_trnc"), trnc_y_scale[which(trnc_y_scale$uni==i),])
  assign(paste0(i, "_log"), log_y_scale[which(log_y_scale$uni==i),])
}

Soc_sci_ctrl <- `Social Sciences_ctrl`
Soc_sci_trnc <- `Social Sciences_trnc`
Soc_sci_log <- `Social Sciences_log`

Sus_geog_ctrl <- `Sustainability/geological science_ctrl`
Sus_geog_trnc <- `Sustainability/geological science_trnc`
Sus_geog_log <- `Sustainability/geological science_log`

stem_ctrl <- rbind(Science_ctrl, Technology_ctrl, Engineering_ctrl, Maths_ctrl, Medicine_ctrl, Architecture_ctrl)
soc_sci_ctrl <- rbind(psychology_ctrl, Soc_sci_ctrl, Sus_geog_ctrl, Criminology_ctrl)
hum_ctrl <- rbind(Humanities_ctrl, Geography_ctrl)

stem_trnc <- rbind(Science_trnc, Technology_trnc, Engineering_trnc, Maths_trnc, Medicine_trnc, Architecture_trnc)
soc_sci_trnc <- rbind(psychology_trnc, Soc_sci_trnc, Sus_geog_trnc, Criminology_trnc)
hum_trnc <- rbind(Humanities_trnc, Geography_trnc)

stem_log <- rbind(Science_log, Technology_log, Engineering_log, Maths_log, Medicine_log, Architecture_log)
soc_sci_log <- rbind(psychology_log, Soc_sci_log, Sus_geog_log, Criminology_log)
hum_log <- rbind(Humanities_log, Geography_log)

con_first <- rbind(ver1[,c(4:7, 10:13)], ver2[,c(4:7, 10:13)])
log_first <- rbind(ver3[,c(4:7, 10:13)], ver4[,c(4:7, 10:13)])
trn_first <- rbind(ver5[,c(4:7, 10:13)], ver6[,c(4:7, 10:13)])

###### ASPECT RATIO ######
def_ratio <- rbind(ver1[,c(4:7, 22:24)], ver2[,c(4:7, 22:24)], ver3[,c(4:7, 25:27)], ver4[,c(4:7, 28:30)], ver5[,c(4:7, 25:27)], ver6[,c(4:7, 28:30)])
nar_ratio <- rbind(ver1[,c(4:7, 25:27)], ver2[,c(4:7, 28:30)], ver3[,c(4:7, 22:24)], ver4[,c(4:7, 22:24)], ver5[,c(4:7, 28:30)], ver6[,c(4:7, 25:27)])
wid_ratio <- rbind(ver1[,c(4:7, 28:30)], ver2[,c(4:7, 25:27)], ver3[,c(4:7, 28:30)], ver4[,c(4:7, 25:27)], ver5[,c(4:7, 22:24)], ver6[,c(4:7, 22:24)])

def_ratio_r <- rbind(r1[,c(4:7, 22:24)], r2[,c(4:7, 22:24)], r3[,c(4:7, 25:27)], r4[,c(4:7, 28:30)], r5[,c(4:7, 25:27)], r6[,c(4:7, 28:30)])
nar_ratio_r <- rbind(r1[,c(4:7, 25:27)], r2[,c(4:7, 28:30)], r3[,c(4:7, 22:24)], r4[,c(4:7, 22:24)], r5[,c(4:7, 28:30)], r6[,c(4:7, 25:27)])
wid_ratio_r <- rbind(r1[,c(4:7, 28:30)], r2[,c(4:7, 25:27)], r3[,c(4:7, 28:30)], r4[,c(4:7, 25:27)], r5[,c(4:7, 22:24)], r6[,c(4:7, 22:24)])

def_ratio_py <- rbind(py1[,c(4:7, 22:24)], py2[,c(4:7, 22:24)], py3[,c(4:7, 25:27)], py4[,c(4:7, 28:30)], py5[,c(4:7, 25:27)], py6[,c(4:7, 28:30)])
nar_ratio_py <- rbind(py1[,c(4:7, 25:27)], py2[,c(4:7, 28:30)], py3[,c(4:7, 22:24)], py4[,c(4:7, 22:24)], py5[,c(4:7, 28:30)], py6[,c(4:7, 25:27)])
wid_ratio_py <- rbind(py1[,c(4:7, 28:30)], py2[,c(4:7, 25:27)], py3[,c(4:7, 28:30)], py4[,c(4:7, 25:27)], py5[,c(4:7, 22:24)], py6[,c(4:7, 22:24)])


for(i in 1:dim(ver1)[1]){
  if(ver1$all_1[i]=="A"){
    ver1$all_1[i] <- "Default"
  }
  if(ver1$all_1[i]=="B"){
    ver1$all_1[i] <- "Narrow"
  }
  if(ver1$all_1[i]=="C"){
    ver1$all_1[i] <- "Wide"
  }
  
  if(ver1$all_2[i]=="A"){
    ver1$all_2[i] <- "Default"
  }
  if(ver1$all_2[i]=="B"){
    ver1$all_2[i] <- "Narrow"
  }
  if(ver1$all_2[i]=="C"){
    ver1$all_2[i] <- "Wide"
  }
  
  if(ver1$all_3[i]=="A"){
    ver1$all_3[i] <- "Default"
  }
  if(ver1$all_3[i]=="B"){
    ver1$all_3[i] <- "Narrow"
  }
  if(ver1$all_3[i]=="C"){
    ver1$all_3[i] <- "Wide"
  }
}

for(i in 1:dim(ver2)[1]){
  if(ver2$all_1[i]=="A"){
    ver2$all_1[i] <- "Default"
  }
  if(ver2$all_1[i]=="C"){
    ver2$all_1[i] <- "Narrow"
  }
  if(ver2$all_1[i]=="B"){
    ver2$all_1[i] <- "Wide"
  }
  
  if(ver2$all_2[i]=="A"){
    ver2$all_2[i] <- "Default"
  }
  if(ver2$all_2[i]=="C"){
    ver2$all_2[i] <- "Narrow"
  }
  if(ver2$all_2[i]=="B"){
    ver2$all_2[i] <- "Wide"
  }
  
  if(ver2$all_3[i]=="A"){
    ver2$all_3[i] <- "Default"
  }
  if(ver2$all_3[i]=="C"){
    ver2$all_3[i] <- "Narrow"
  }
  if(ver2$all_3[i]=="B"){
    ver2$all_3[i] <- "Wide"
  }
}

for(i in 1:dim(ver3)[1]){
  if(ver3$all_1[i]=="B"){
    ver3$all_1[i] <- "Default"
  }
  if(ver3$all_1[i]=="A"){
    ver3$all_1[i] <- "Narrow"
  }
  if(ver3$all_1[i]=="C"){
    ver3$all_1[i] <- "Wide"
  }
  
  if(ver3$all_2[i]=="B"){
    ver3$all_2[i] <- "Default"
  }
  if(ver3$all_2[i]=="A"){
    ver3$all_2[i] <- "Narrow"
  }
  if(ver3$all_2[i]=="C"){
    ver3$all_2[i] <- "Wide"
  }
  
  if(ver3$all_3[i]=="B"){
    ver3$all_3[i] <- "Default"
  }
  if(ver3$all_3[i]=="A"){
    ver3$all_3[i] <- "Narrow"
  }
  if(ver3$all_3[i]=="C"){
    ver3$all_3[i] <- "Wide"
  }
}

for(i in 1:dim(ver4)[1]){
  if(ver4$all_1[i]=="B"){
    ver4$all_1[i] <- "Default"
  }
  if(ver4$all_1[i]=="C"){
    ver4$all_1[i] <- "Narrow"
  }
  if(ver4$all_1[i]=="A"){
    ver4$all_1[i] <- "Wide"
  }
  
  if(ver4$all_2[i]=="B"){
    ver4$all_2[i] <- "Default"
  }
  if(ver4$all_2[i]=="C"){
    ver4$all_2[i] <- "Narrow"
  }
  if(ver4$all_2[i]=="A"){
    ver4$all_2[i] <- "Wide"
  }
  
  if(ver4$all_3[i]=="B"){
    ver4$all_3[i] <- "Default"
  }
  if(ver4$all_3[i]=="C"){
    ver4$all_3[i] <- "Narrow"
  }
  if(ver4$all_3[i]=="A"){
    ver4$all_3[i] <- "Wide"
  }
}

ver5$all_1[is.na(ver5$all_1)] <- "none"

for(i in 1:dim(ver5)[1]){
  
  if(ver5$all_1[i]=="C"){
    ver5$all_1[i] <- "Default"
  }
  if(ver5$all_1[i]=="A"){
    ver5$all_1[i] <- "Narrow"
  }
  if(ver5$all_1[i]=="B"){
    ver5$all_1[i] <- "Wide"
  }
  
  if(ver5$all_2[i]=="C"){
    ver5$all_2[i] <- "Default"
  }
  if(ver5$all_2[i]=="A"){
    ver5$all_2[i] <- "Narrow"
  }
  if(ver5$all_2[i]=="B"){
    ver5$all_2[i] <- "Wide"
  }
  
  if(ver5$all_3[i]=="C"){
    ver5$all_3[i] <- "Default"
  }
  if(ver5$all_3[i]=="A"){
    ver5$all_3[i] <- "Narrow"
  }
  if(ver5$all_3[i]=="B"){
    ver5$all_3[i] <- "Wide"
  }
}

for(i in 1:dim(ver6)[1]){
  if(ver6$all_1[i]=="C"){
    ver6$all_1[i] <- "Default"
  }
  if(ver6$all_1[i]=="B"){
    ver6$all_1[i] <- "Narrow"
  }
  if(ver6$all_1[i]=="A"){
    ver6$all_1[i] <- "Wide"
  }
  
  if(ver6$all_2[i]=="C"){
    ver6$all_2[i] <- "Default"
  }
  if(ver6$all_2[i]=="B"){
    ver6$all_2[i] <- "Narrow"
  }
  if(ver6$all_2[i]=="A"){
    ver6$all_2[i] <- "Wide"
  }
  
  if(ver6$all_3[i]=="C"){
    ver6$all_3[i] <- "Default"
  }
  if(ver6$all_3[i]=="B"){
    ver6$all_3[i] <- "Narrow"
  }
  if(ver6$all_3[i]=="A"){
    ver6$all_3[i] <- "Wide"
  }
}

for(i in c(unique(def_ratio$uni), unique(nar_ratio$uni), unique(wid_ratio$uni))){
  assign(paste0(i, "_def"), def_ratio[which(def_ratio$uni==i),])
  assign(paste0(i, "_nar"), nar_ratio[which(nar_ratio$uni==i),])
  assign(paste0(i, "_wid"), wid_ratio[which(wid_ratio$uni==i),])
}

Soc_sci_def <- `Social Sciences_def`
Soc_sci_nar <- `Social Sciences_nar`
Soc_sci_wid <- `Social Sciences_wid`

Sus_geog_def <- `Sustainability/geological science_def`
Sus_geog_nar <- `Sustainability/geological science_nar`
Sus_geog_wid <- `Sustainability/geological science_wid`

stem_def <- rbind(Science_def, Technology_def, Engineering_def, Maths_def, Medicine_def, Architecture_def)
soc_sci_def <- rbind(psychology_def, Soc_sci_def, Sus_geog_def, Criminology_def)
hum_def <- rbind(Humanities_def, Geography_def)

stem_nar <- rbind(Science_nar, Technology_nar, Engineering_nar, Maths_nar, Medicine_nar, Architecture_nar)
soc_sci_nar <- rbind(psychology_nar, Soc_sci_nar, Sus_geog_nar, Criminology_nar)
hum_nar <- rbind(Humanities_nar, Geography_nar)

stem_wid <- rbind(Science_wid, Technology_wid, Engineering_wid, Maths_wid, Medicine_wid, Architecture_wid)
soc_sci_wid <- rbind(psychology_wid, Soc_sci_wid, Sus_geog_wid, Criminology_wid)
hum_wid <- rbind(Humanities_wid, Geography_wid)


comp_ratio_all <- rbind(ver1[,c(4:7, 31:33)], ver2[,c(4:7, 31:33)], ver3[,c(4:7, 31:33)], ver4[,c(4:7, 31:33)], ver5[,c(4:7, 31:33)], ver6[,c(4:7, 31:33)])

comp_ratio_r <- rbind(r1[,c(4:7, 31:33)], r2[,c(4:7, 31:33)], r3[,c(4:7, 31:33)], r4[,c(4:7, 31:33)], r5[,c(4:7, 31:33)], r6[,c(4:7, 31:33)])

comp_ratio_py <- rbind(py1[,c(4:7, 31:33)], py2[,c(4:7, 31:33)], py3[,c(4:7, 31:33)], py4[,c(4:7, 31:33)], py5[,c(4:7, 31:33)], py6[,c(4:7, 31:33)])
 
def_first <- rbind(ver1[,c(4:7, 22:24)], ver2[,c(4:7, 22:24)])
nar_first <- rbind(ver3[,c(4:7, 22:24)], ver4[,c(4:7, 22:24)])
wid_first <- rbind(ver5[,c(4:7, 22:24)], ver6[,c(4:7, 22:24)])

###### STACKED ######
vir_stacked <- rbind(ver1[,c(4:7, 34:37)], ver5[,c(4:7, 38:41)])
def_stacked <- rbind(ver2[,c(4:7, 34:37)], ver3[,c(4:7, 38:41)])
gr_stacked <- rbind(ver4[,c(4:7, 34:37)], ver6[,c(4:7, 38:41)])

vir_grouped <- rbind(ver1[,c(4:7, 38:41)], ver5[,c(4:7, 34:37)])
def_grouped <- rbind(ver2[,c(4:7, 38:41)], ver3[,c(4:7, 34:37)])
gr_grouped <- rbind(ver4[,c(4:7, 38:41)], ver6[,c(4:7, 34:37)])

vir_stacked_r <- rbind(r1[,c(4:7, 34:37)], r5[,c(4:7, 38:41)])
def_stacked_r <- rbind(r2[,c(4:7, 34:37)], r3[,c(4:7, 38:41)])
gr_stacked_r <- rbind(r4[,c(4:7, 34:37)], r6[,c(4:7, 38:41)])

vir_grouped_r <- rbind(r1[,c(4:7, 38:41)], r5[,c(4:7, 34:37)])
def_grouped_r <- rbind(r2[,c(4:7, 38:41)], r3[,c(4:7, 34:37)])
gr_grouped_r <- rbind(r4[,c(4:7, 38:41)], r6[,c(4:7, 34:37)])

vir_stacked_py <- rbind(py1[,c(4:7, 34:37)], py5[,c(4:7, 38:41)])
def_stacked_py <- rbind(py2[,c(4:7, 34:37)], py3[,c(4:7, 38:41)])
gr_stacked_py <- rbind(py4[,c(4:7, 34:37)], py6[,c(4:7, 38:41)])

vir_grouped_py <- rbind(py1[,c(4:7, 38:41)], py5[,c(4:7, 34:37)])
def_grouped_py <- rbind(py2[,c(4:7, 38:41)], py3[,c(4:7, 34:37)])
gr_grouped_py <- rbind(py4[,c(4:7, 38:41)], py6[,c(4:7, 34:37)])

set_a <- ver1[,c(4:9, 42:44)]
set_b <- ver2[,c(4:9, 42:44)]
set_c <- ver3[,c(4:9, 42:44)]
set_d <- ver4[,c(4:9, 42:44)]
set_e <- ver5[,c(4:9, 42:44)]
set_f <- ver6[,c(4:9, 42:44)]

set_a_r <- r1[,c(4:9, 42:44)]
set_b_r <- r2[,c(4:9, 42:44)]
set_c_r <- r3[,c(4:9, 42:44)]
set_d_r <- r4[,c(4:9, 42:44)]
set_e_r <- r5[,c(4:9, 42:44)]
set_f_r <- r6[,c(4:9, 42:44)]

set_a_py <- py1[,c(4:9, 42:44)]
set_b_py <- py2[,c(4:9, 42:44)]
set_c_py <- py3[,c(4:9, 42:44)]
set_d_py <- py4[,c(4:9, 42:44)]
set_e_py <- py5[,c(4:9, 42:44)]
set_f_py <- py6[,c(4:9, 42:44)]

###### SALES_AB ######
ab_sep <- rbind(ver1[,c(4:7, 45:47)], ver2[,c(4:7, 45:47)], ver3[,c(4:7, 48:50)], ver4[,c(4:7, 51:53)], ver5[,c(4:7, 48:50)], ver6[,c(4:7, 51:53)])

ab_trn <- rbind(ver1[,c(4:7, 48:50)], ver2[,c(4:7, 51:53)], ver3[,c(4:7, 45:47)], ver4[,c(4:7, 45:47)], ver5[,c(4:7, 51:53)], ver6[,c(4:7, 48:50)])

ab_zero <- rbind(ver1[,c(4:7, 51:53)], ver2[,c(4:7, 48:50)], ver3[,c(4:7, 51:53)], ver4[,c(4:7, 48:50)], ver5[,c(4:7, 45:47)], ver6[,c(4:7, 45:47)])


###### SALES_CD ######
cd_trn <- rbind(ver1[,c(4:7, 54)], ver2[,c(4:7, 54)], ver3[,c(4:7, 55)], ver4[,c(4:7, 54)], ver5[,c(4:7, 55)], ver6[,c(4:7, 55)])

cd_zro <- rbind(ver1[,c(4:7, 55)], ver2[,c(4:7, 55)], ver3[,c(4:7, 54)], ver4[,c(4:7, 55)], ver5[,c(4:7, 54)], ver6[,c(4:7, 54)])
```

This chapter will discuss basic univariate analysis of the survey results, including summary statistics and univariate testing for the whole population as well as the subsetting for the programming language used and degree type. Additionally, subsets will be created considering only the first plot shown for each question, drawing comparisons between responses for these plots themselves without influence of the others. 

In terms of testing, Shapiro-Wilk tests will be applied to gauge whether the data sets can be considered normally distributed and thus whether parametric T-Tests are suitable for either one-sample or paired comparisons. Failing he normality condition, a symmetry test will be administered, and providing there is sufficient evidence to say the data is symmetric, a Mann-Whitney-Wilcoxon (MWW) test will be used. If there is insufficient evidence that data proves either symmetric or normally distributed, sign tests will be applied. MWW will also be used for two sample testing where perhaps a sign test would be most appropriate, but cannot be used as the samples are of different sizes.

\subsection{Ninja Warrior Part 1 - Y-axis Scaling}
As a recap, the first part of the survey consisted of showing the respondents three differently scaled bar plots representing how many times four obstacles were used throughout 10 seasons of the television show American Ninja Warrior. The three presented visualisations all showed the same raw data, but each was produced with a different y-axis scaling, in order to assess whether changing the scale in these ways affects viewer interpretation of interpreting both differences and exact bar heights. 

```{r}
control_1 <- ctrl_y_scale$con_1
control_1[which(control_1 == "41/42")] <- # take midpoint of two values
control_1 <- na.exclude(as.numeric(control_1))

truncated_1 <- as.numeric(trnc_y_scale$trn_1)

logarithmic_1 <- log_y_scale$log_1
logarithmic_1[which(logarithmic_1 == "Don't know")] <- NA
logarithmic_1[which(logarithmic_1 == "Next to none.")] <- NA
logarithmic_1[which(logarithmic_1 == "10^15")] <- NA
logarithmic_1[which(logarithmic_1 == "10^9")] <- NA
logarithmic_1[which(logarithmic_1 == "1000.0")] <- NA
logarithmic_1[which(logarithmic_1 == "1000")] <- NA
logarithmic_1 <- as.numeric(na.exclude(logarithmic_1))

y_scale_1_all <- cbind(control_1, truncated_1, logarithmic_1)
```

The first question of the survey, \textit{"Approximately many times would you
say the ‘Salmon Ladder’ was used?"} asked participants to type the how many times Salmon Ladder was used, based on the bar plot. The `correct' answer, or rather the true height of the corresponding bar, was 41. There were three invalid answers in these responses; one for the R versions of '41/42', and two for the Python versions, given as 'Don't know' and 'Next to none.'. These will be considered as 'NA' responses and discounted from the quantitative analysis, however they do provide useful qualitative insights into how the respondents reacted to the plots, particularly as both were entered for the logarithmically scaled plot made in Python. 

The sets of responses regarding the control and truncated plots have means 41.21 and 41.35 respectively and both have median 41, showing that as compared to the default scaling, the truncated scale didn't much alter the respondents' perception of the bar's height. Performing sign tests with the null hypothesis of the true location being equal to 41, it is seen that the responses for the control plot do not significantly differ from the true location at a $5\%$ level of significance, with $p = 0.1214$. The truncated plot responses are picked up to differ in location to the true value at this significance level, at $p =  0.002563$. However, a dependent samples sign tests reveals that there is insufficient evidence ($p = 0.1877$) to suggest a significant difference in the locations of the two samples. The control and truncated plots have contextually fairly small variances of 0.752 and 0.753 respectively, depicting both that there is limited variation in the responses and most of the observations lie fairly close to the respective means. The variances are also quite similar, showing that the distributions appear fairly similar, as emphasised by observing the below density plot and box plots.

```{r fig.cap = "Density plot showing distributions of responses regarding the control and truncated plots"}

brks <- c("Control", "Truncated")
vals <- c("#1c9e77", "#d95f02")

ggplot() +
  geom_density(data = as.data.frame(control_1), aes(x=control_1, col = "Control"))+
  geom_density(data = as.data.frame(truncated_1), aes(x=truncated_1, col = "Truncated"))+
  labs(x="Response", y="Density")+
  scale_colour_manual(name = " ", breaks = brks, values = vals)+
  theme_classic()
```

```{r fig.cap = "Box plots showing distributions of responses regarding the control and truncated plots"}
y_scale_1_all <- as.data.frame(y_scale_1_all)
resp <- c(control_1, truncated_1)
type <- c(rep('Control', 69), rep('Truncated', 70)) 
stats_1 <- data.frame(resp, type)

ggplot(data = stats_1, aes(x=type, y=resp, fill=type))+
geom_boxplot(outlier.colour="black", outlier.shape=1,
             outlier.size=2, notch=F)+
  theme_classic()+
  stat_summary(fun=mean, geom="point", shape=15, size=4)+
  scale_fill_manual(breaks = brks, values = vals)+
  scale_y_continuous(labels = seq(1, 7, 1), breaks = seq(1, 7, 1))+
  ylab("Value")+
  xlab("Plot Type")
```

The mean of the responses for the logarithmically-scaled plot, on the other hand, was magnitudes higher at 1.493e+13, although with a median of 35; lower than the median response of the control and truncated plots responses. The high magnitude is the result of two answers of '10^15' and '10^9', both for the python version of the plot. Additionally, the two invalid text answers as mentioned above were given for the Python version.

The default logarithmic scaling in Python uses standard form notation, which perhaps the two participants who entered the high magnitude answers were less exposed to and not as familiar with. Looking at the degree subjects for these respondents, it is observed that they study Social Sciences and Psychology, respectively. This could add to the idea that they are less familiar with this notation as it is more commonly used in mathematical and physical science disciplines. One of the respondents also rated their numerical skills at 1/5, showing they feel that numerical skill is not their specialty. The other rated their numeric skills at 4/5, showing that even with a good self-perceived level of numerical skill, standard form could be considered misleading.

This should perhaps be considered when designing visualisations; the creator of the visualisations may find the logarithmic scale or standard form more effective in showing the data, but they should consider the target audience. Are the audience going to be familiar with this? If, for example, visualisations are being published in a paper targeted at academics in a subject likely to use such scalings often and understand them, this may be a good way to depict the data. However, using this in something such as an advertising campaign could mislead the public, causing them to either over or under estimate values. As previously discussed, however, this is often done deliberately in order to push the message the creator wishes to sell. 

The variance in the responses for the logarithmic plot is also high, with value $1.492 \times 10^{28}$, showing that a large amount of the observations differ from the very high mean, and considering this alongside the lower median may point towards many of the respondents either giving an accurate response or even underestimating. Furthering this point, the IQR for the logarithmic responses is the interval $[30, 40]$, which sits below the true value, displaying that over 50% of the observations in the total population actually underestimate the value.

When considering the statistics and distributions for the R and Python subsets, the language used seems to have little effect on the distributions of responses in the control and truncated plot responses; the medians for the control and truncated plot responses stay consistent at 41, the true value, and the means are once again all similar (41.49 and 41.57 for R, 40.87 and 41.10 for Python). MWW tests comparing the languages for the control and truncated plots, however, reveal that there is in fact a statistically significant difference in location of the data ($p=0.0001$ for control and $p=0.02$ for truncated). These tests compared first the R and Python responses for the control plot responses, and then for the truncated plot responses. This test is not ideal for this data, however, since it is not symmetric. We conclude that the locations of the data do differ by testing, but we see by eye that this difference is minimal. 

Considering the responses related to the logarithmic plots, there is actually a slight underestimation in the R version (mean = 39.73, median = 35, IQR = [35, 40]) and, as expected, vast overestimation for the Python version. This shows that, with a linearly notated logarithmic scale, the scale may cause underestimation, but this is counteracted by using a standard form notation. 

Compared to the control scaled plot and based on the information discussed to far, it appears that truncating the y-scale has marginally improved the respondents' ability to judge the height of the bar. This makes sense as truncating the scale results in less of a spread of values on the same sized scale, and so wider gap between each value and thus potentially allows a more accurate judging of values. 

Note that there is a discrepancy here between languages in terms of the axis labeling, with the R plot being incremented in steps of 10 for both the control and truncated plots and the Python being more granular in steps of 5 for the control and steps of 2.5 for the truncated, although here this doesn't appear to have had too much of an impact since, as discussed before, the distributions are fairly similar by eye despite the significant result of testing. 

Now consider the response distribution for the logarithmically-scaled plot, after removing the two responses of "10^15" and "10^9", as well as the values at 1000.

```{r, fig.cap = "Density plot showing distributions of responses regarding the logarithmic scaled plot, after removing values of greater or equal to 1000"}
ggplot() +
  geom_density(data = as.data.frame(logarithmic_1), aes(x=logarithmic_1, col = "Logarithmic"))+
  labs(x="Response", y="Density")+
  scale_colour_manual(name = " ", breaks = "Logarithmic", values = '#7570b3')+
  theme_classic()

```
After removing the outlying values, taken as any value above 1000, the distribution is positively skewed and centres around 40 with a tail up to 150. The summary statistics for the outlier removed set shows a mean of 37.45, with a range of $[9, 150]$ and median of 35. The IQR shows that $50\%$ of these values lie in the range $[22.5, 40]$, meaning there is potentially have a greater underestimation than for the other two plots, also shown by the mean.

Overall, it appears that the use of the truncated scale had little impact on judging the height of an individual bar as compared to a control with no scale alterations.

Now considering the logarithmic-scale plot, The Python default of standard form notation appears to have confused certain respondents, who are perhaps not as used to seeing this notation, and there was a very large range in the responses along with one person not even entering a number, but rather stating that they "Don't know", and another stating they believed the value was "Next to none". The "Next to none" entry is very subjective, but could potentially be be assumed as a value close to 0, once again maybe as a result of standard form being less well known to this respondent.

Finally, comparing the distributions between the three sets of responses, for which we only consider responses for the first shown plots, we see similar distributions to the whole population, showing that the plot order has mininmal impact here. 

**Approximately how much more than 'Log Grip' would you say 'Salmon Ladder' was was used?**

Now consider the results from the second question, in which the participants were asked to respond on a scale from 1-7. The bar plot below depicts the distribution of results for each of the three plot types.


```{r}
control_2 <- ctrl_y_scale$con_2
truncated_2 <- trnc_y_scale$trn_2
logarithmic_2 <- log_y_scale$log_2

y_scale_2_all <- cbind(control_2, truncated_2, logarithmic_2)
y_scale_2_all <- as.data.frame(y_scale_2_all)
resp <- c(control_2, truncated_2, logarithmic_2)
type <- c(rep('Control', 70), rep('Truncated', 70), rep('Logarithmic', 70)) 
stats_2 <- data.frame(resp, type)

ggplot(stats_2)+
  geom_bar(aes(x=resp, group = type, fill = type), position = 'dodge') +
  ylab("Number of Respondents")+
  xlab("Response")+
  labs(title = "Responses selected Over the Whole Population")+
  scale_fill_brewer(palette="Dark2", labels = c('Control', 'Logarithmic','Truncated'))+
  scale_y_continuous(breaks = seq(0, 15, 3), labels = seq(0, 15, 3))+
  scale_x_discrete(breaks = c("1", "2", "3", "4", "5", "6", "7"), labels = c("1", "2", "3", "4", "5", "6", "7"), limits = c("1", "2", "3", "4", "5", "6", "7"))
```
The spread of logarithmic plot values is fairly wide, with at least one response for each option, while the spread for the truncated and control plot responses are more skewed to the right, depicting that the subjective view on the difference between the bar heights was that the difference was on the larger side.

An initial look at the table of summary statistics reveal means of 5.375, 3.671 and 5.871 respectively for the control, truncated and logarithmic plots, meaning that for the 'baseline' control plot, participants on average judged the difference to be moderately significant, with the perceived difference being smaller for the log plot and marginally larger for the truncated plot. This appears to be consistent with results from the @YANG2021, in which the researchers, similar to this survey, showed participants a series of control bar plots alongside those with a truncated axis, and concluded that the difference in values for the truncated axis were perceived to be larger than those of the control plots. However, the average perceived difference here is fairly small, much smaller than initially hypothesised, so tests will be needed to decipher whether this is significant.

The logarithmic plot causing the average perceived difference to be smaller than the control follows the hypothesis from prior to running the survey. 

The interquartile range for the control plot is smallest of the three at 1.75, followed by the truncated plot at 2, and then the log plot at 2.75. This depicts that overall, there was more of a consensus in the subjective perception of the difference for the control plot than the other two, and less agreement between participants for the logarithmic scale. 

Using sign tests, the difference in the control and truncated, and control and logarithmic responses are significant ($p=0.00019$ and $p=2.047e-11$), but this is much more significant for the logarithmic. An MWW test shows that the difference in location for the logarithmic and truncated is also highly significant ($p=6.669e-13$). 

A comparison between languages for each plot type reveals that the language has limited effect on the responses for the control and truncated plots ($ $ and $ $), but the location for the 

**Approximately how much more than 'Quintuple Steps' would you say 'Salmon Ladder' was used?**

This is a similar question to the one prior, but the purpose was to see if there was a difference in perceived difference for bars next to each other vs bars on opposite sides of the plot.


```{r}
control <- ctrl_y_scale$con_3
truncated <- trnc_y_scale$trn_3
logarithmic <- log_y_scale$log_3

y_scale_3_all <- cbind(control, logarithmic, truncated)

ggplot() +
  geom_density(data = as.data.frame(control), aes(x=control, col = "Control"))+
  geom_density(data = as.data.frame(logarithmic), aes(x=logarithmic, col = "Logarithmic"))+
  geom_density(data = as.data.frame(truncated), aes(x=truncated, col = "Truncated"))+
  labs(x="Response", y="Density")+
  scale_colour_manual(name = " ", breaks = brks, values = vals)+
  theme_classic()
```

The distributions here appear more regular than for the previous question, with the truncated plot appearing almost normal, with the control plot skewed slightly more to the left, and then the logarithmic skewed once again more to the left. The means for these three, in the same order, are 3.771, 3.129 and 2.229, respectively, with medians of 4, 3 and 2. The ranges for these are all similar, with the truncated plot responses sitting in $[2, 7]$, the control responses in $[1, 7]$, and the logarithmic in $[1, 6]$, with variances of 1.309, 1.157 and 1.599. All of this points towards the respondents perceiving the differences in the truncated plot as lower than the control plot, and the logarithmic plot higher. This is again confirmed in the below box plots.
```{r, echo=F}
y_scale_3_all <- as.data.frame(y_scale_3_all)
resp <- c(y_scale_3_all$control, y_scale_3_all$logarithmic, y_scale_3_all$truncated)
type <- c(rep('Control', 70), rep('Truncated', 70), rep('Logarithmic', 70)) 
stats_3 <- data.frame(resp, type)

ggplot(data = stats_3, aes(x=type, y=resp, fill=type))+
geom_boxplot(outlier.colour="black", outlier.shape=1,
             outlier.size=2, notch=F)+
  theme_classic()+
  stat_summary(fun=mean, geom="point", shape=15, size=4)+
  scale_fill_manual(breaks = brks, values = vals)+
  scale_y_continuous(labels = seq(1, 7, 1), breaks = seq(1, 7, 1))+
  ylab("Value")+
  xlab("Plot Type")

```

This is similar to the conclusions from the previous question.

Now consider at the means for both of the two questions regarding bar height difference perception, and compare the mean responses for the logarithmic and truncated plots with to the mean of the control plot responses. This will be in the form of taking the difference between the mean control response and mean of the truncated or logarithmic responses, respectively. This will give an idea of how much of a impact each scaling makes has on perception as compared to the control plot, depending on if the bars are next to each other or further apart. The table below gives these differences, alongside a bar plot giving a visual representation of these.

```{r}
control <- ctrl_y_scale$con_2
truncated <- trnc_y_scale$trn_2
logarithmic <- log_y_scale$log_2

y_scale_2_all <- cbind(control, truncated, logarithmic)

con_mean_2 <-  mean(control)
trn_mean_2 <- mean(truncated)
log_mean_2 <- mean(logarithmic)
means_2 <- c(trn_mean_2, log_mean_2)

control <- ctrl_y_scale$con_3
truncated <- trnc_y_scale$trn_3
logarithmic <- log_y_scale$log_3

y_scale_3_all <- cbind(control, truncated, logarithmic)

con_mean_3 <-  mean(control)
trn_mean_3 <- mean(truncated)
log_mean_3 <- mean(logarithmic)
means_3 <- c(trn_mean_3, log_mean_3)


diff_mat <- matrix(NA, 2, 2)

for(i in 1:2){
  diff_mat[1, i] <- con_mean_2-means_2[i]
  diff_mat[2, i] <- con_mean_3-means_3[i]
}
```

```{r}
colnames(diff_mat) <- c("Con - Trnc", "Con - Log")
rownames(diff_mat) <- c("Log Grip", "Quintuple Steps")
kable(diff_mat, caption = "Table showing difference in the percieved difference for the logarithmic-scaled and truncated plots as compared to the control, for the two above questions") %>%
  kable_styling(latex_options = "hold_position")
```



```{r fig.cap= "Bar plot giving a visual representation of the table"}
df <- data.frame("Trnc"=c(diff_mat[,1]), "Log"=c(diff_mat[,2]), "Question" = c("Log Grip vs Salmon Ladder", "Quintuple Steps vs Salmon Ladder"))


ggplot()+
  geom_bar(data=df, aes(x = Question, y=Trnc, fill = brks[2]), stat="identity")+
  geom_bar(data=df, aes(x = Question, y=Log, fill = brks[3]), stat="identity")+
  geom_hline(yintercept = 0)+
  scale_fill_manual(name="Scale" ,breaks=brks[2:3], values=vals[2:3])+
  scale_y_continuous(name = "Average difference from control plot", limits = c(-1, 2), breaks = seq(-1, 2, 0.5))+
  theme_classic()
```

On average, truncating the scale had a similar effect for both questions, albeit with slightly more effect for when comparing 'Salmon Ladder' with 'Quintuple Steps' as opposed to 'Log Grip'. For the logarithmically scaled plots, however, the re-scaling appears to have had a significantly greater effect when considering the bars directly next to each other, with respondents on average judging the difference in bar height to be greater by 1.68 on the 7-point scale, whereas this is 0.9 for the bars further apart. It can be concluded from this that truncating the scale had more of an impact when bars were on opposite ends of the plot as opposed to next to each other, and the way round for the bars close to each other; the logarithmic scaling had more of an impact.

\subsection{Ninja Warrior - Part 2}


**How large would you say the difference between 'Jumping spider' and 'Salmon Ladder' is?**

This question once again uses the 7-point scale to gain a subjective view on the degree to which respondents felt the heights between the two bars corresponding to 'Jumping Spider' and 'Salmon Ladder' differed for three bar plots of 7 obstacles, where 'Salmon Ladder' is furthest to the left, and 'Jumping Spider' furthest to the right.

```{r}
default <- def_ratio$def_1
narrower <- nar_ratio$nar_1
wider <- wid_ratio$wid_1
ratio_1_all <- cbind(default, wider, narrower)
```
Looking at the means and medians here, it doesn't seem like there is that much of a difference in perception of the differences between the three aspect ratios. With means of 5.914, 5.357 and 6.129 for the default narrower and wider plots respectively, where 'narrower' is defined as the plot with the aspect ratio of smaller width to greater height, and vice versa for the 'wider' plot, and all have a median of 6. The means show marginal differences, whereby the default plot mean is in the middle-valued mean, with the mean perceived difference for the wider plot being slightly smaller than this and the mean perceived difference for the narrower plot is slightly larger. This result, although marginal, follows the hypothesis that the wider plot would cause differences to be perceived as smaller and narrower bars to cause differences to be perceived to be greater.

To discuss the ranges, see the box plot below.

```{r, echo=F}

brks <- c("Default", "Wider", "Narrower")
vals <- c("#1c9e77", "#d95f02", "#7570b3")

ratio_1_all <- as.data.frame(ratio_1_all)
resp <- c(default, wider, narrower)
type <- c(rep('Default', 70), rep('Wider', 70), rep('Narrower', 70)) 
stats <- data.frame(resp, type)

ggplot(data = stats, aes(x=type, y=resp, fill=type))+
  geom_boxplot(outlier.colour="black", outlier.shape=1,
             outlier.size=2, notch=F)+
  theme_classic()+
  stat_summary(fun=mean, geom="point", shape=15, size=4)+
  scale_fill_manual(breaks = brks, values = vals)+
  scale_y_continuous(labels = seq(1, 7, 1), breaks = seq(1, 7, 1))+
  ylab("Value")+
  xlab("Plot Type")

```
From these box plots, it appears that the IQRs for the two plots with altered aspect ratios have very little, if any, overlap, despite the means being similar and medians being identical. The narrower plot shows a tendency for the responses to lie more towards the upper end of the scale than the wider plot, which also ranges over the upper half of the scale but between roughly 5 and 6 rather than 6 and 7. The default plot covers the entire IQR of both of the other plots, and the box plots then show that, even though the means and medians are very similar, the center bulk of the values for the narrower plot tended to be more towards the upper end of the default's IQR, whereas the central points of the wider plot sat on the lower half of this IQR. Additionally, we see that there are two outliers each for the narrower and wider plots, with values 2, 3 and 4. Perhaps excluding these from the data and re-analysing the summary statistics we will see a more marked difference.



```{r}
default <- def_ratio$def_1

narrower <- nar_ratio$nar_1[which(nar_ratio$nar_1 != 3)]
narrower <- narrower[which(narrower != 4)]

wider <- wid_ratio$wid_1[which(wid_ratio$wid_1 != 2)]
wider <- wider[which(wider != 3)]

ratio_1_all <- cbind(default, wider, narrower)
```

After performing the univariate analysis excluding these values, the means and medians didn't change that much, with the medians still at 6 for all the plots, and the mean response for the wider plot increasing from 5.357 to 5.543, and the mean response for the narrower plot from 6.129 to 6.257. Although as expected, the ranges and variances are lower, meaning the spread over values over the range is smaller, however this only furthers the point that altering the axis ratio appears to have minimal effect.This is again confirmed by looking at the distributions of the three plot types, which are very similar to one another. We can however see that the plot with the density for the wider plot is highest of the three for the values of 4 and 5, but is the lowest of the three for upper end of the distributions, and vice versa for the densities narrower plot. The default mostly stays in between the other two curves. For a third time, this gives way to the observation that the wider bars have a small lessening effect on gauging differences in height, and using narrower bars has a mild increasing effect on difference perception.

```{r}

ggplot() +
  geom_density(data = as.data.frame(default ), aes(x=default , col = "Default"))+
  geom_density(data = as.data.frame(wider   ), aes(x=wider   , col = "Wider"))+
  geom_density(data = as.data.frame(narrower), aes(x=narrower, col = "Narrower"))+
  labs(x="Response", y="Density")+
  scale_colour_manual(name = " ", breaks = brks, values = vals)+
  theme_classic()

```


**How large would you say the difference between 'Log Grip' and 'Floating Steps' is?**

Similar to part 1, we have two questions for gauging differences between bars, for which one asks about bars far away from each other, and one about bars next to each other. In the case of this section, the first question contained bars on opposite ends of the x-axis, and this question asks about two bars that sit adjacent to one another. 


```{r}
default <- def_ratio$def_2
narrower <- nar_ratio$nar_2
wider <- wid_ratio$wid_2

ratio_2_all <- cbind(default, wider, narrower)
```
The analysis results here show that altering the axis ratio appears to have even less of an effect than in the first question, with the means of the responses for the default and wider plots being identical at 3.057, with the mean of the narrower plot responses only 0.157 greater at3.214. The median for all three is 3, and the IQRs are all $[2, 7]$. The variances, however, do differ from one another, with values 1.301, 0.866 and 1.214 for the default, wider and narrower bars, respectively. The distribution of values are shown in the below box plots.

```{r, echo=F}

brks <- c("Default", "Wider", "Narrower")
vals <- c("#1c9e77", "#d95f02", "#7570b3")

ratio_1_all <- as.data.frame(ratio_2_all)
resp <- c(default, wider, narrower)
type <- c(rep('Default', 70), rep('Wider', 70), rep('Narrower', 70)) 
stats <- data.frame(resp, type)

ggplot(data = stats, aes(x=type, y=resp, fill=type))+
  geom_boxplot(outlier.colour="black", outlier.shape=1,
             outlier.size=2, notch=F)+
  theme_classic()+
  stat_summary(fun=mean, geom="point", shape=15, size=4)+
  scale_fill_manual(breaks = brks, values = vals)+
  scale_y_continuous(labels = seq(1, 7, 1), breaks = seq(1, 7, 1))+
  ylab("Value")+
  xlab("Plot Type")

```
We see that at least $50/%$ of respondents placed the difference in the range $[2, 4]$ for all three plots, showing that they believed the difference was small to moderate, and this didn't change depending on the plot type, and thus for the bars further apart from each other, changing the aspect ratio does not appear to make much of a difference. The overall distributions are shown in the below density plots   


```{r}

ggplot() +
  geom_density(data = as.data.frame(default ), aes(x=default , col = "Default"))+
  geom_density(data = as.data.frame(wider   ), aes(x=wider   , col = "Wider"))+
  geom_density(data = as.data.frame(narrower), aes(x=narrower, col = "Narrower"))+
  labs(x="Response", y="Density")+
  scale_x_continuous(limits=c(0, 7))+
  scale_colour_manual(name = " ", breaks = brks, values = vals)+
  theme_classic()

```

We see all three distributions are very similar, and almost appear to form bell curve shaped distributions, albeit with some irregularities and very slight negative skew.

Once again, we can analyse the distance from the perceived bar height difference for the default aspect ratio to those of the wider and narrower ratios, for each of the two height difference questions

```{r}
default <- def_ratio$def_1
narrower <- nar_ratio$nar_1
wider <- wid_ratio$wid_1
ratio_1_all <- cbind(default, wider, narrower)

def_mean_1 <-  mean(default)
nar_mean_1 <- mean(narrower)
wid_mean_1 <- mean(wider)
means_1 <- c(nar_mean_1, wid_mean_1)

default <- def_ratio$def_2
narrower <- nar_ratio$nar_2
wider <- wid_ratio$wid_2
ratio_2_all <- cbind(default, wider, narrower)

def_mean_2 <-  mean(default)
nar_mean_2 <- mean(narrower)
wid_mean_2 <- mean(wider)
means_2 <- c(nar_mean_2, wid_mean_2)

diff_mat <- matrix(NA, 2, 2)

for(i in 1:2){
  diff_mat[1, i] <- def_mean_1-means_1[i]
  diff_mat[2, i] <- def_mean_2-means_2[i]
}
```

```{r}
colnames(diff_mat) <- c("Def - Narrow", "Def - Wide")
rownames(diff_mat) <- c("Jumping Spider vs Salmon Ladder", "Log Grip vs Floating Steps")
kable(diff_mat, caption = "Table showing difference in the percieved difference for plots with narrower and wider bars as compared to the default, for the two above questions") %>%
  kable_styling(latex_options = "hold_position")
```



```{r fig.cap= "Bar plot giving a visual representation of the table"}
df <- data.frame("Narrower"=c(diff_mat[,1]), "Wider"=c(diff_mat[,2]), "Question" = c("Jumping Spider vs Salmon Ladder", "Log Grip vs Floating Steps"))


ggplot()+
  geom_bar(data=df, aes(x = Question, y=Narrower, fill = brks[2]), stat="identity")+
  geom_bar(data=df, aes(x = Question, y=Wider, fill = brks[3]), stat="identity")+
  geom_hline(yintercept = 0)+
  scale_fill_manual(name="Scale" ,breaks=brks[2:3], values=vals[2:3])+
  scale_y_continuous(name = "Average difference from default plot", limits = c(-0.25, 0.6), breaks = seq(-0.25, 0.6, 0.1))+
  theme_classic()
```

**How many times would you say 'Floating Steps' were used?**

This is again similar to question 1 of part 1, where participants were asked to state what they believed to be the height of the bar for 'Salmon Ladder', however this time we choose the third bar from the axis. This is to ascertain whether the distance of the bar from the axis may have an effect alongside any potential perceived distortion of values. Note that the true value was 28.

```{r}
default <- def_ratio$def_3
narrower <- nar_ratio$nar_3
wider <- wid_ratio$wid_3

ratio_1_all <- cbind(default, wider, narrower)
```
The means of each of the three sets of responses were very close to the true value, at 27.97, 28.04 and 27.39, respectively for the default, wider and narrower, and the medians are exactly equal to the true value. Based on the means and medians it appears that, once again, altering the aspect ratio had minimal, if any, effect on interpretation of the data value. The value for the default plot also appears to be closer to the true value than the control plot in part 1, question 1.

```{r, echo=F}

brks <- c("Default", "Wider", "Narrower")
vals <- c("#1c9e77", "#d95f02", "#7570b3")

resp <- c(default, wider, narrower)
type <- c(rep('Default', 70), rep('Wider', 70), rep('Narrower', 70)) 
stats <- data.frame(resp, type)

ggplot(data = stats, aes(x=type, y=resp, fill=type))+
  geom_boxplot(outlier.colour="black", outlier.shape=1,
             outlier.size=2, notch=F)+
  theme_classic()+
  stat_summary(fun=mean, geom="point", shape=15, size=4)+
  scale_fill_manual(breaks = brks, values = vals)+
  ylab("Value")+
  xlab("Plot Type")

```

Looking at the box plots, we see very small ranges in the values, signifying that there was a large consensus between respondents in terms of what they perceived the height to be. It can also be seen that there are three outliers below the box plot for the narrower plot responses, and two above for the default plot responses. There is very little overlap between the boxes, and it appears again that there altering the aspect ratio of the bar plot has little to no impact on reading the height of the bar. Additionally, there was less agreement between respondents for the wider plot than for the other two, although this doesn't seem to be too significant.  


```{r}

ggplot() +
  geom_density(data = as.data.frame(default ), aes(x=default , col = "Default"))+
  geom_density(data = as.data.frame(wider   ), aes(x=wider   , col = "Wider"))+
  geom_density(data = as.data.frame(narrower), aes(x=narrower, col = "Narrower"))+
  labs(x="Response", y="Density")+
  scale_colour_manual(name = " ", breaks = brks, values = vals)+
  theme_classic()
```
The distributions for the default and narrower plot responses are very similar, both seeming to be fairly centred on the mean with a steep decrease in density on either side of the mean to very shallow tails within the range $[25, 30]$. The responses for the wider plot appear to be more spread with lower density function values, with a slight negative skew.

```{r}
default <- default[-which(default >= 30)]
narrower <- narrower[-which(narrower <= 25)]

ratio_1_all <- cbind(default, wider, narrower)
```
After removing the outliers the medians have stayed the same, and the mean has obviously decreased for the default and increased for the narrower, however, these means are all still fairly similar to each other and at a first glance prior to testing it again seems that changing the aspect ratio, at least to the degree tested here, is inconsequential to interpretation of the actual value. As expected as well, the variances for the outlier-removed sets have decreased.

**Comparisons**

The last set of questions in part 2 show respondents all three of the bar plots presented in this section and ask them to select which they find most aesthetically pleasing, and which they find easiest and hardest to interpret. Below a table is laid out giving the number of respondents that selected each plot for each of the three questions.

```{r echo=F}
comp_1 <- comp_ratio$all_1
comp_1 <- comp_1[-which(comp_1 == "none")]
comp_2 <- comp_ratio$all_2
comp_3 <- comp_ratio$all_3
tab <- rbind(table(comp_1), table(comp_2), table(comp_3))
rownames(tab) <- c("Most aesthetically pleasing?", "Easiest to read and interpret?", "Hardest to read and interpret?")
tab
```

For the first question, relating to how aesthetically pleasing respondents found each plot, just over half of the respondents chose the default aspect ratio as the most aesthetically pleasing, with 37 out of the 69 who responded selecting this. 

Similarly, 37 out of the 70 that responded to the second question found the plot with the default aspect ratio easiest to read and interpret. Perhaps the people that preferred this aspect ratio aesthetically did so because they found it easiest to interpret. Investigating this, we find that 27 who chose the default for question 1 also chose this for question 2.

The plot judged hardest to read and interpret by the most respondents was the one with the wider bars, with 30 selecting this and 20 selecting each of the other two. While a significant number chose the default and narrower bars, the slightly higher amount selecting the plot with wider bars matches the previously stated hypothesis formulated from following the Stephen Few paper, which discusses that an ratio of greater width to length could suffer from perceptual imbalance. While we don't see this imbalance in the numbers from the previous questions, the result here does give some indication that the aspect ratio producing wider bars may impact on ease of interpretation.


\subsection{Ninja Warrior - Part 3}

The third and final part of the questions about the American Ninja Warrior data discusses stacked bars and colour schemes. The questions asked in this part are used to decipher how data with multiple categories may be best represented in a bar plot. The plots presented use the same bars as in part 1, but this time we highlight the number of times each obstacle was used in each stage of the competition for each bar.  Each participant was shown both a stacked and a grouped bar plot in one of three colour schemes; the default for the language, viridis, and greyscale. For three versions of the survey, the stacked bars were shown first, and for the other three versions the first shown was the grouped bars. The final question of this part also asked respondents to compare two colour schemes, and through the 6 surveys we have comparisons of every colour scheme against every other colour scheme.

**How many times would you say 'Floating Steps' were used in the Finals (Regional/City) round?**

Again we start with the less subjective question regarding the reading of a numerical value off the axis. In this question we ask about 'Floating Steps', which is the bar third along from the y-axis. The question asks respondents to view the bar plot, where the bars will either be grouped of stacked, and decipher how many times this obstacle was used in the specified round of the competition. The true value for this was 11. The hypothesis for this question is that the respondents will more accurately gauge the value for the grouped bar than the stacked, which as we see below appears to be the case.

```{r}
vir_stacked_1 <- vir_stacked$vir_sta_1
def_stacked_1 <- def_stacked$def_sta_1
gr_stacked_1 <- gr_stacked$gr_sta_1

vir_grouped_1 <- vir_stacked$vir_grp_1
def_grouped_1 <- def_stacked$def_grp_1
gr_grouped_1 <- gr_stacked$gr_grp_1

stacked_1 <- c(vir_stacked_1, def_stacked_1, gr_stacked_1)
grouped_1 <- c(vir_grouped_1, def_grouped_1, gr_grouped_1)

bars <- data.frame(stacked_1, grouped_1)
```
The mean for the values estimated by respondents using the stacked bars is 14.32, a fair bit larger than the true value of 11, and the mean estimated value for the grouped bars was closer to the true value, at 11.8. The IQR for the grouped bars is also smaller than for the stacked, and comprises of the range $[11, 12]$, insinuating that the estimated values tended to be fairly accurate but with some respondents perhaps slightly overestimating. The IQR for the stacked bars on the other hand covers the interval $[10, 14]$, which does contain the true value, but shows a tendency for both over and underestimation of respondents. Additionally to this, there is a very large variance in the responses to this question, at 54.8 compared to the variance of 13.1 for the responses regarding the grouped bar plots. This adds to the picture that there was much less agreement between respondents, with many straying away from the mean of 14.3. We do see however that the median for both the stacked and grouped bars is 11, showing that the higher mean of the stacked bars may be a result of an influential value at the upper end of the distribution, and that many observations do actually sit around 11. The fact that many values actually sit around 11 could be contributing to the higher variance, as variance is simply the sum of the squared distances from the mean, and so will be elevated if there are many values that sit some distance away from the mean. The higher mean could be reflected in the maximum of the stacked responses being 35, although the maximum of the grouped responses is 40, so there may be more than one influential point in the stacked responses. We can check for outliers by looking at the box plots for this data.

```{r}
brks <- c("Stacked", "Grouped")
vals <- c("#1c9e77", "#d95f02", "#7570b3")

resp <- c(stacked_1, grouped_1)
type <- c(rep('Stacked', 70), rep('Grouped', 70)) 
stats_2 <- data.frame(resp, type)

ggplot(data = stats_2, aes(x=type, y=resp, fill=type))+
geom_boxplot(outlier.colour="black", outlier.shape=1,
             outlier.size=2, notch=F)+
  theme_classic()+
  stat_summary(fun=mean, geom="point", shape=15, size=4)+
  scale_fill_manual(breaks = brks, values = vals)+
  ylab("Value")+
  xlab("Plot Type")
```
We do in fact see that the box for the grouped responses is very short and centered around 11. The box for the stacked responses shows many high valued outliers that could be causing the mean to be higher, although the IQR is still a fair bit larger than that of the responses for the grouped bars. The mean for this also sits above the IQR, and thus the outliers may be having a significant influence. Now we will remove the outliers assuming, from the box plot, that outliers are any values above or equal to 25 for the stacked responses and above or equal to 20 for the grouped. 

```{r}

stacked_1 <- c(vir_stacked_1, def_stacked_1, gr_stacked_1)
grouped_1 <- c(vir_grouped_1, def_grouped_1, gr_grouped_1)
```
We see that removing the outliers as specified by the box plot, the mean of the stacked responses is now just above 11, and actually closer to the true value than the mean of the other set of responses, and the median has decreased to 10. From this one could infer that there is no difference between each type of bar plot in terms of gauging the size of the bars. However, we see that there are 12 outliers in the stacked responses, which leads to the idea that these are not in fact all outliers and may be valid responses that just sit on the upper end of the distribution. However, it seems the cause of the high values could be respondents taking the whole height of the bar, which has an actual height of 28, rather than the section of interest. Many of the potentially influential values fall around the range $[25, 30]$, with all but 2 of the 12 potential outliers sitting in this interval, with the remaining two both being 35. Looking below at the summary statistics for only the values picked up as outliers, we see a mean of 29.83, which is higher than the true value of 28, and interestingly goes against the analysis from part 1, question 2 whereby respondents were asked to judge the height of this bar and on average underestimated. The fact that so many participants misinterpreted this plot and signify that stacked bar plots may not be the best way to present data to general public, as there may be the potential to misread the height of the whole bar as the size of the top category.


```{r}

vir_stacked_1 <- vir_stacked$vir_sta_1
def_stacked_1 <- def_stacked$def_sta_1
gr_stacked_1 <- gr_stacked$gr_sta_1

vir_grouped_1 <- vir_stacked$vir_grp_1
def_grouped_1 <- def_stacked$def_grp_1
gr_grouped_1 <- gr_stacked$gr_grp_1

stacked_1 <- c(vir_stacked_1, def_stacked_1, gr_stacked_1)
grouped_1 <- c(vir_grouped_1, def_grouped_1, gr_grouped_1)

stacked_1 <- stacked_1[which(stacked_1 >= 25)] 
```
As a result of this, we will discount this set of 12 values from the analysis, and thus come to the conclusion that, for the respondents that appear to have judged the height of the correct section, there was little to no impact when using stacked vs grouped bar charts, and most of the difference comes from misinterpretation of the plot itself, as opposed to a poorer judgment of size.

To see if either of these values are significantly far from the true value, we once again run tests. Firstly, running Shapiro tests and symmetry tests to check for violations, we see that this data is not normal and is asymmetric. Thus, as in question 1, we run sign tests, alongside the t-tests on samples from a normal distribution with mean and variance equal to out data. We will test against a median of 11 for the sign tests, and a mean of 11 for the t-tests. 

The sign test on the stacked bar plot responses gives a high p-value of 0.5258, showing that for the stacked bar plot responses (after removing the values as priorly specified), the participant estimated values do not differ significantly from the true value. For the grouped bar plot we have a p value of 0.009 < 0.05, and thus these responses are statistically significantly different from the true value. 

Running t-tests on the means, however, we see both sets of responses differ statistically significantly from the true value.

**How many times would you say 'Log Grip' was used in the Finals (Regional/City) round?**

This question is similar the above, but for the next bar to the right. The purpose of this question was to test the same hypothesis as the previous question, and also to lead into the following question, where respondents were asked to compare the 'Floating Steps' and 'Log Grip'. Additionally, the bar in the previous question had only two categories, of which the respondents were asked to judge the size of the category on the top of the bar in the stacked plot, whereas the bar for 'Log Grip' has 5 categories, of which the category of interest sits above 4. The true value of this was 9.

```{r}
vir_stacked_2 <- vir_stacked$vir_sta_2
def_stacked_2 <- def_stacked$def_sta_2
gr_stacked_2 <- gr_stacked$gr_sta_2

vir_grouped_2 <- vir_stacked$vir_grp_2
def_grouped_2 <- def_stacked$def_grp_2
gr_grouped_2 <- gr_stacked$gr_grp_2

stacked_2 <- c(vir_stacked_2, def_stacked_2, gr_stacked_2)
grouped_2 <- c(vir_grouped_2, def_grouped_2, gr_grouped_2)

bars <- data.frame(stacked_2, grouped_2)
```
Similarly to the previous question, the mean response for the stacked bar plots are higher than that of the grouped, and the mean of the stacked also slightly overestimates the value. Once again however, we appear to see a selection of respondents judging the full height of the bar rather than the category as asked. Looking at the data, the interval for these responses seems to be $[20, 25]$, as the next response below 20 is a value of 10, seeming to separate the data into two separate subsets. This can be confirmed by a box plot. 

```{r}
brks <- c("Stacked", "Grouped")
vals <- c("#1c9e77", "#d95f02", "#7570b3")

resp <- c(stacked_2, grouped_2)
type <- c(rep('Stacked', 70), rep('Grouped', 70)) 
stats_2 <- data.frame(resp, type)

ggplot(data = stats_2, aes(x=type, y=resp, fill=type))+
geom_boxplot(outlier.colour="black", outlier.shape=1,
             outlier.size=2, notch=F)+
  theme_classic()+
  stat_summary(fun=mean, geom="point", shape=15, size=4)+
  scale_fill_manual(breaks = brks, values = vals)+
  ylab("Value")+
  xlab("Plot Type")
```
We indeed see that the distribution of values for each of the two response sets appears to be almost identical with the exception of outliers at and above 20 for the box plot of responses for the stacked bar plot. Thus we view the sets of summary statistics for the two but with these values removed. 

```{r}
stacked_2 <- c(vir_stacked_2, def_stacked_2, gr_stacked_2)
grouped_2 <- c(vir_grouped_2, def_grouped_2, gr_grouped_2)
```
Here we see that there tended to be a slight underestimation in the value for the stacked bar plot, however this is approximately 0.46 away from the true value, and unlikely to be significant. This can again be tested as above, where it is less clear whether the data are symmetric, so we will also run symmetry test.

Once again the response sets are non-normally distributed and asymmetric, and so sign tests are applicable. The response set for the stacked bar plots produces a p-value of around 0.04, which shows a statistically significant difference in the responses from the true value of 9 at the 0.05 level of significance. However, this would very easily become insignificant by slightly lowering the significance level to, say, 0.035. The p-value for the grouped bar responses, however, is >> 0.05, as expected given that the median of the data sits at the true value.

The t-tests show that the differences in the means from the true value are statistically significant, although not considering the tests we can see by eye that the means are relatively close to 9.


**Please select the statement you feel applies to the bar chart above.**

This question asked respondents to judge whether log grip was used more, less, or an equal amount in the Finals (Regional/City) and Qualifying(Regional/City) rounds. This was to see how well differences between sizes of categories are judged when relating to the same variable, and are in the same bar. The results for this are given in the table below.

```{r}
vir_stacked_3 <- vir_stacked$vir_sta_3
def_stacked_3 <- def_stacked$def_sta_3
gr_stacked_3 <- gr_stacked$gr_sta_3

vir_grouped_3 <- vir_stacked$vir_grp_3
def_grouped_3 <- def_stacked$def_grp_3
gr_grouped_3 <- gr_stacked$gr_grp_3

stacked_3 <- c(vir_stacked_3, def_stacked_3, gr_stacked_3)
grouped_3 <- c(vir_grouped_3, def_grouped_3, gr_grouped_3)
```
The table shows overwhelmingly that significantly more people accurately judged that the two values were the same for the grouped bars than for the stacked bars. This was the hypothesised result, and has presented to an  even greater extent than previously anticipated. All but 7 of the respondents who responded to this question correctly judged from the grouped bars that the obstacle was used an equal number of times in each of the two rounds, whereas the responses for the grouped bar seemed fairly well split between the three options. It may be interesting in the multivariate analysis section to compare responses depending on whether respondents were shown the stacked or grouped bars first. Perhaps a reason for the incorrect judging with the stacked


**Which obstacle do you think was used MORE in Finals (Regional/City) rounds, 'Log Grip' or 'Floating Steps'?**

Similar to the previous question, this asks for a comparison between the size of two categories, but this time about how many times two different obstacles were used in the round Finals (Regional/City), where these two obstacles are those discussed at the start of this part of the survey.

```{r}
vir_stacked_4 <- vir_stacked$vir_sta_4
def_stacked_4 <- def_stacked$def_sta_4
gr_stacked_4 <- gr_stacked$gr_sta_4

vir_grouped_4 <- vir_stacked$vir_grp_4
def_grouped_4 <- def_stacked$def_grp_4
gr_grouped_4 <- gr_stacked$gr_grp_4

stacked_4 <- c(vir_stacked_4, def_stacked_4, gr_stacked_4)
grouped_4 <- c(vir_grouped_4, def_grouped_4, gr_grouped_4)
```
This was a potentially poorly formulated question, as the respondents had already been asked to specify how many times each of these obstacle was used in this round and respondents mostly judged this accurately with regard to both plots, but this could have been impacted by the previous questions. However, this does follow from the results from the past questions showing that respondents mostly accurately judged the values correctly, aside from those who instead judged the height of the whole bar.

**Which bar chart do you feel is easiest to read and interpret?**

Here was simply assess the perceived ease of interpretation of both bar plots. This is to gain an understanding in how data may best be presented in an easily understandable, easily readable manner. This is an important factor in visualisation, as a main aim in creating visuals is to provide an aid for the viewer to simply and quickly see the message. The opposite may be beneficial in certain applications however; based on the misreadings in the question regarding judging the number of times 'Log Grip' was used in the specific round, viewers of the visualisations could be easily mislead by incorrectly interpreting the plot. The people being shown the plot in, for example, an advert, may only take a fleeting look and not go beyond to analyse the plot to see accurate differences between values, and thus it is important to produce a plot that gives the easiest interpretation.

```{r}
a_1 <- set_a$sta_dge
b_1 <- set_b$sta_dge
c_1 <- set_c$sta_dge
d_1 <- set_d$sta_dge
e_1 <- set_e$sta_dge
f_1 <- set_f$sta_dge

for(i in 1:length(a_1)){
  if(a_1[i] == "A"){
    a_1[i] <- "Stacked"
  } else a_1[i] <- "Grouped"
}

for(i in 1:length(b_1)){
  if(b_1[i] == "A"){
    b_1[i] <- "Stacked"
  } else b_1[i] <- "Grouped"
}

for(i in 1:length(d_1)){ 
  if(d_1[i] == "A"){
    d_1[i] <- "Stacked"
  } else d_1[i] <- "Grouped"
}

for(i in 1:length(c_1)){
  if(c_1[i] == "B"){
    c_1[i] <- "Stacked"
  } else c_1[i] <- "Grouped"
}

for(i in 1:length(e_1)){
  if(e_1[i] == "B"){
    e_1[i] <- "Stacked"
  } else e_1[i] <- "Grouped"
}

for(i in 1:length(f_1)){
  if(f_1[i] == "B"){
    f_1[i] <- "Stacked"
  } else f_1[i] <- "Grouped"
}

tab <- table(c(a_1, b_1, c_1, d_1, e_1, f_1))
kable(tab)
```
The large majority of participants found the grouped bar chart easier to read and interpret, as predicted.


**Which colour scheme do you find most aesthetically pleasing?**

This question and the one following it are asked with the purpose of assessing the colour scheme that gives the greatest aesthetic pleasure, or effectively which colour palette the respondents feel is subjectively the 'prettiest' or 'nicest'. It is important to note here that aesthetics and readability do not always go hand-in-hand; a plot that is made to look very aesthetically pleasing may sacrifice readability, and vice versa. For each of the two languages we created six pairings of three different colour palettes, whereby the first colour was the one displayed for the main questions, and the second used only for the comparison questions. As previously discussed, the three colour schemes considered are viridis, greyscale, and each language's default plotting colour palette. The colour palette pairings are outlined below.

```{r}

set <- c("A", "B", "C", "D", "E", "F")
first_col <- c("Viridis", "Default", "Default", "Greyscale", "Viridis", "Greyscale")
second_col <- c("Default", "Viridis", "Greyscale", "Default", "Greyscale", "Viridis")

df <- data.frame(set, first_col, second_col)
names(df) <- c("Pairing ID", "Main Colour Palette", "Secondary Colour Pallette")
knitr::kable(df)
```


```{r echo=F}
a_1 <- set_a$a_cols_1
b_1 <- set_b$b_cols_1
c_1 <- set_c$c_cols_1
d_1 <- set_d$d_cols_1
e_1 <- set_e$e_cols_1
f_1 <- set_f$f_cols_1

tab <- rbind(table(a_1), table(b_1), table(c_1), table(d_1), c(table(e_1), 0), table(f_1))
rownames(tab) <- c("Set A", "Set B", "Set C", "Set D", "Set E", "Set F")
kable(tab)
```
This table shows that when it came to the default/viridis pairings, displayed in the first two rows, the respondents tended to have no preference overall, although this may differ between languages, which will be explored later on. Comparing this to the bottom two rows, in which we put viridis against greyscale, only 1 respondent out of the 23, a proportion of 0.04, found the grey more aesthetically pleasing, as hypothesised. When considering greyscale/default, there was still a majority preferring the non-greyscale palette, but a higher proportion preferred this as compared to the viridis/greyscale, with 4 out of the 22, or a proportion of 0.18, preferring the grey.

**Do you feel that one of the colour schemes makes it easier to read and interpret? If so, please select which one.**

Complementing the aesthetic preferences, this question assesses the colour preference with regard to readability and ease of interpretation. As mentioned before, this will be used to test both the colour palette preference itself alongside whether this preference matches up with aesthetic preference.

```{r, fig.cap="Table showing which colour palettes respondents found easiest to read and interpret"}
a_1 <- set_a$a_cols_2
b_1 <- set_b$b_cols_2
c_1 <- set_c$c_cols_2
d_1 <- set_d$d_cols_2
e_1 <- set_e$e_cols_2
f_1 <- set_f$f_cols_2

for(i in 1:length(a_1)){
  if(a_1[i] == "Yes, A is easier"){
    a_1[i] <- "A"
  } else if(a_1[i] == "Yes, A is easier"){
  a_1[i] <- "B"
  } else a_1[i] <- "None"
}

for(i in 1:length(b_1)){
  if(b_1[i] == "Yes, B is easier"){
    b_1[i] <- "B"
  } else if(b_1[i] == "Yes, A is easier"){ 
    b_1[i] <- "A"
    }else b_1[i] <- "None"
}

for(i in 1:length(e_1)){
  if(e_1[i] == "Yes, A is easier"){
    e_1[i] <- "A"
  } else if(e_1[i] == "Yes, B is easier"){
    e_1[i] <- "B"
  } else e_1[i] <- "None"
}

for(i in 1:length(f_1)){
  if(f_1[i] == "Yes, B is easier"){
    f_1[i] <- "B"
  } else if(f_1[i] == "Yes, A is easier"){
    f_1[i] <- "A"
  } else f_1[i] <- "None"
}

for(i in 1:length(c_1)){
  if(c_1[i] == "Yes, A is easier"){
    c_1[i] <- "A"
  } else if(c_1[i] == "Yes, B is easier"){ 
    c_1[i] <- "B"
  } else c_1[i] <- "None"
}

for(i in 1:length(d_1)){
  if(d_1[i] == "Yes, A is easier"){
    d_1[i] <- "A"
  } else if(d_1[i] == "Yes, B is easier"){ 
    d_1[i] <- "B"
  } else d_1[i] <- "None"
}

tab <- table(c(a_1, b_1, c_1, d_1, e_1, f_1))
tab
```
Interestingly here, we see that the top two rows appear to give opposing results; the respondents who were presented with viridis for the main questions and the default as a secondary palette stated that they found either viridis easier to interpret or had no preference, whereas those presented with the default first and viridis second tended to find the default easier. Once again looking at the comparisons with the greyscale, there were some respondents that found this easier to read, but the majority chose the alternative, whether this is viridis or the default.

\subsection{Sales - Part 1}
Now we move on to the sales part of the survey. In this section data was taken from a the `BJsales` data set in R, which is a time series data set containing 150 observations. This data set constitutes a single vector of values with no specified timings, and the visualisation data was formed by taking subsets of size 12 this and setting a month between each point to give a year of fictional sales data.

**How much would you say sales of each company increased between January and December? [Company A]**

This question was included for the purpose of testing whether, again, axis scaling impacts the perceived differences between values, but this time with time series line plots as opposed to bar plots. Respondents were asked to assess how much the sales of company A increased over the course of the year, or in other words to look at and compare each end of the line. 

```{r}
sep_1a <- na.exclude(ab_sep$sep_1a)
trn_1a <- ab_trn$ab_trn_1a
zro_1a <- ab_zero$ab_zro_1

for(i in 1:length(sep_1a)){
  if(sep_1a[i] == "1 (A little)"){
    sep_1a[i] <- "1"
  }
  if(sep_1a[i] == "7 (A lot)"){
    sep_1a[i] <- "7"
  }
}

for(i in 1:length(trn_1a)){
  if(trn_1a[i] == "1 (A little)"){
    trn_1a[i] <- "1"
  }
  if(trn_1a[i] == "7 (A lot)"){
    trn_1a[i] <- "7"
  }
}

for(i in 1:length(zro_1a)){
  if(zro_1a[i] == "1 (A little)"){
    zro_1a[i] <- "1"
  }
  if(zro_1a[i] == "7 (A lot)"){
    zro_1a[i] <- "7"
  }
}

sep_1a <- as.numeric(sep_1a)
trn_1a <- as.numeric(trn_1a)
zro_1a <- as.numeric(zro_1a)
```


The plot for which the respondents, on average, found the difference to be smallest was the zeroed, followed by the truncated, and then the separated, with means of 1.371, 2.414 and 3.043 respectively. Furthermore, the zeroed plot has a small overall range, spanning $[1, 3]$ of the scale. The other two plots have range $[1, 7]$, but IQRs of $[2, 4]$ and $[2, 3]$.

**How much would you say sales of each company increased between January and December? [Company B]**

```{r}
sep_1b <- na.exclude(ab_sep$sep_1b)
trn_1b <- na.exclude(ab_trn$ab_trn_1b)
zro_1b <- na.exclude(ab_zero$ab_zro_1b)

for(i in 1:length(sep_1b)){
  if(sep_1b[i] == "1 (A little)"){
    sep_1b[i] <- "1"
  }
  if(sep_1b[i] == "7 (A lot)"){
    sep_1b[i] <- "7"
  }
}

for(i in 1:length(trn_1b)){
  if(trn_1b[i] == "1 (A little)"){
    trn_1b[i] <- "1"
  }
  if(trn_1b[i] == "7 (A lot)"){
    trn_1b[i] <- "7"
  }
}

for(i in 1:length(zro_1b)){
  if(zro_1b[i] == "1 (A little)"){
    zro_1b[i] <- "1"
  }
  if(zro_1b[i] == "7 (A lot)"){
    zro_1b[i] <- "7"
  }
}

sep_1b <- as.numeric(sep_1b)
trn_1b <- as.numeric(trn_1b)
zro_1b <- as.numeric(zro_1b)
```


**How large would you say the drop in sales between April and July of Company A  is?**

```{r}
sep_2 <- na.exclude(ab_sep$sep_2)
trn_2 <- na.exclude(ab_trn$ab_trn_2)
zro_2 <- na.exclude(ab_zero$ab_zro_2)
```

\subsection{Sales - Part 2}

**Based on the above graph, how large would you say the difference is between the number of sales Company C makes and the number of sales Company D makes?**


```{r}
trn_cd <- na.exclude(cd_trn$cd_trn)
zro_cd <- na.exclude(cd_zro$cd_zro)
```



\subsection{Conclusions}

















